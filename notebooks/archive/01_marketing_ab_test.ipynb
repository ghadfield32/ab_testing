{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing A/B Test Analysis: Complete Walkthrough\n",
    "\n",
    "This notebook demonstrates a real-world marketing A/B test using the Marketing dataset (588K users).\n",
    "\n",
    "**Scenario**: E-commerce company testing new ad creatives\n",
    "\n",
    "**Business Question**: Should we switch to the new ad design?\n",
    "\n",
    "**Primary Metric**: Conversion rate (binary outcome)\n",
    "\n",
    "**Dataset**: faviovaz/marketing-ab-testing from Kaggle (588,101 observations)\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. ‚úÖ Data quality validation and SRM checks\n",
    "2. ‚úÖ Power analysis and sample size calculations\n",
    "3. ‚úÖ CUPED variance reduction with pre-experiment covariates\n",
    "4. ‚úÖ Guardrail metrics and non-inferiority testing\n",
    "5. ‚úÖ Novelty effect detection\n",
    "6. ‚úÖ Ship/hold/abandon decision framework\n",
    "7. ‚úÖ Business impact translation\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to validate experiment randomization (SRM check)\n",
    "- When your experiment has enough power to detect effects\n",
    "- How CUPED can speed up your experiments by 20-40%\n",
    "- How to protect guardrail metrics while optimizing primary metrics\n",
    "- How to detect and handle novelty effects\n",
    "- How to make data-driven ship/hold/abandon decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AB Testing package imports\n",
    "from ab_testing.data import loaders\n",
    "from ab_testing.pipelines.marketing_pipeline import run_marketing_analysis\n",
    "from ab_testing.core import power as power_module\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üìä Ready to analyze Marketing A/B test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Load and Inspect Data\n",
    "\n",
    "### üìö Why This Matters\n",
    "\n",
    "**Always inspect data before analysis**. Real-world datasets have issues:\n",
    "- Missing values\n",
    "- Duplicates\n",
    "- Outliers\n",
    "- Type mismatches\n",
    "\n",
    "**Bad data ‚Üí Bad decisions**. Netflix, Booking.com, and Meta all run automated data quality checks before every experiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "print(\"Loading Marketing A/B Test dataset...\")\n",
    "df = loaders.load_marketing_ab()\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(df):,} observations\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   ‚úÖ No missing values detected\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(f\"\\n2. Data Types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(f\"\\n3. Control vs Treatment Split:\")\n",
    "split = df['test_group'].value_counts()\n",
    "display(split)\n",
    "ratio = split['treatment'] / split['control']\n",
    "print(f\"\\n   Ratio: {ratio:.4f} (should be ~1.0 for 50/50 split)\")\n",
    "if abs(ratio - 1.0) < 0.05:\n",
    "    print(\"   ‚úÖ Split looks balanced\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Split appears imbalanced - check SRM in next step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"CONVERSION RATES BY GROUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "conversion_summary = df.groupby('test_group')['converted'].agg([\n",
    "    ('Total Users', 'count'),\n",
    "    ('Conversions', 'sum'),\n",
    "    ('Conversion Rate', 'mean')\n",
    "])\n",
    "\n",
    "display(conversion_summary)\n",
    "\n",
    "control_rate = conversion_summary.loc['control', 'Conversion Rate']\n",
    "treatment_rate = conversion_summary.loc['treatment', 'Conversion Rate']\n",
    "absolute_diff = treatment_rate - control_rate\n",
    "relative_diff = (treatment_rate / control_rate - 1) * 100\n",
    "\n",
    "print(f\"\\nüìä Quick Analysis:\")\n",
    "print(f\"   Control conversion: {control_rate:.4%}\")\n",
    "print(f\"   Treatment conversion: {treatment_rate:.4%}\")\n",
    "print(f\"   Absolute difference: {absolute_diff:.4%} ({absolute_diff*100:.2f} percentage points)\")\n",
    "print(f\"   Relative lift: {relative_diff:.2f}%\")\n",
    "print(f\"\\n   {'‚úÖ Treatment looks better!' if relative_diff > 0 else '‚ùå Treatment looks worse'}\")\n",
    "print(f\"   (But we need statistical testing to confirm!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Run Complete Analysis Pipeline\n",
    "\n",
    "Now we'll run the full analysis pipeline which performs all 8 steps:\n",
    "1. Data validation\n",
    "2. SRM check\n",
    "3. Power analysis\n",
    "4. Primary test (Z-test for proportions)\n",
    "5. CUPED variance reduction\n",
    "6. Guardrail metrics\n",
    "7. Novelty detection\n",
    "8. Decision framework\n",
    "\n",
    "We'll run it with `verbose=False` to get structured results, then examine each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete analysis pipeline\n",
    "print(\"Running full Marketing A/B analysis pipeline...\")\n",
    "print(\"(This may take 30-60 seconds)\\n\")\n",
    "\n",
    "results = run_marketing_analysis(sample_frac=1.0, verbose=False)\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(f\"\\nAvailable results: {list(results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Step-by-Step Results Analysis\n",
    "\n",
    "### Step 1: Sample Ratio Mismatch (SRM) Check\n",
    "\n",
    "#### üìö What is SRM?\n",
    "\n",
    "**Sample Ratio Mismatch (SRM)** occurs when the actual group sizes don't match the expected ratio.\n",
    "\n",
    "**Example**: You expect 50/50 split, but get 52/48.\n",
    "\n",
    "**Why It Matters**: SRM indicates randomization failure ‚Üí ALL subsequent results are INVALID.\n",
    "\n",
    "**Common Causes**:\n",
    "- Implementation bugs in randomization code\n",
    "- Telemetry/tracking issues (some users not logged)\n",
    "- Bot traffic\n",
    "- Browser compatibility issues\n",
    "\n",
    "**Industry Standard**: If SRM detected ‚Üí STOP experiment, fix bug, restart. Don't analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: SAMPLE RATIO MISMATCH (SRM) CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "srm = results['srm_check']\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"   Chi-square statistic: {srm['test_statistic']:.4f}\")\n",
    "print(f\"   P-value: {srm['p_value']:.6f}\")\n",
    "print(f\"   Threshold (alpha): 0.01\")\n",
    "print(f\"   SRM detected: {srm['srm_detected']}\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETATION:\")\n",
    "if srm['srm_detected']:\n",
    "    print(f\"   ‚ö†Ô∏è  SRM DETECTED (p < 0.01)\")\n",
    "    print(f\"   ‚ö†Ô∏è  DO NOT TRUST ANY RESULTS - INVESTIGATE IMMEDIATELY\")\n",
    "    print(f\"   ‚ö†Ô∏è  Check: randomization code, tracking pixels, bot filters\")\n",
    "    print(f\"\\n   üè¢ What companies do:\")\n",
    "    print(f\"      - Netflix: Stops ALL analysis if SRM detected\")\n",
    "    print(f\"      - Booking.com: Uses alpha=0.001 (even stricter)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No SRM detected (p = {srm['p_value']:.4f} > 0.01)\")\n",
    "    print(f\"   ‚úÖ Randomization appears valid - safe to proceed\")\n",
    "    print(f\"\\n   üè¢ Industry best practice:\")\n",
    "    print(f\"      - Always check SRM BEFORE any other analysis\")\n",
    "    print(f\"      - Use alpha=0.01 (stricter than typical 0.05)\")\n",
    "    print(f\"      - Even small SRM can indicate serious problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Power Analysis\n",
    "\n",
    "#### üìö What is Statistical Power?\n",
    "\n",
    "**Power** = Probability of detecting an effect if it truly exists\n",
    "\n",
    "**Typical Target**: 80% power (industry standard)\n",
    "\n",
    "**Why It Matters**: Underpowered tests miss real effects (false negatives)\n",
    "\n",
    "**Key Inputs**:\n",
    "- **Baseline rate**: Current conversion rate (e.g., 5%)\n",
    "- **MDE (Minimum Detectable Effect)**: Smallest change you care about (e.g., 2% relative = 5.0% ‚Üí 5.1%)\n",
    "- **Alpha**: False positive rate (typically 0.05)\n",
    "- **Power**: Desired detection probability (typically 0.80)\n",
    "\n",
    "**Output**: Required sample size per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 2: POWER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "power = results['power_analysis']\n",
    "\n",
    "print(f\"\\nüìä Input Parameters:\")\n",
    "print(f\"   Baseline conversion rate: {power['p_baseline']:.4%}\")\n",
    "print(f\"   MDE (relative): {power['mde']:.1%}\")\n",
    "print(f\"   Alpha (false positive rate): {power['alpha']:.2f}\")\n",
    "print(f\"   Power (detection probability): {power['power']:.0%}\")\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Cohen's h effect size: {power['cohens_h']:.4f}\")\n",
    "print(f\"   Required sample per group: {power['sample_per_group']:,}\")\n",
    "print(f\"   Current sample per group: {power['current_sample']:,}\")\n",
    "\n",
    "ratio = power['current_sample'] / power['sample_per_group']\n",
    "print(f\"   Sample ratio: {ratio:.2f}x required\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETATION:\")\n",
    "if ratio >= 1.0:\n",
    "    print(f\"   ‚úÖ WELL-POWERED (have {ratio:.1f}x required sample)\")\n",
    "    print(f\"   ‚úÖ Can detect effects as small as {power['mde']:.1%} with {power['power']:.0%} confidence\")\n",
    "    print(f\"   ‚úÖ Low risk of false negatives (missing real effects)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  UNDERPOWERED (have {ratio:.1%} of required sample)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Risk of false negatives (missing real effects)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Recommendation: Extend experiment or increase traffic\")\n",
    "\n",
    "print(f\"\\nüè¢ Industry Standards:\")\n",
    "print(f\"   - Meta: 80% power for primary metric, 50%+ for guardrails\")\n",
    "print(f\"   - Typical MDE: 1-5% relative lift (depends on baseline rate)\")\n",
    "print(f\"   - Always run power analysis BEFORE starting experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî¨ Interactive Exercise: Try Different MDEs\n",
    "\n",
    "Let's explore how MDE affects required sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ EXPERIMENT: How MDE Affects Sample Size\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "mdes = [0.01, 0.02, 0.05, 0.10, 0.20]  # 1%, 2%, 5%, 10%, 20% relative\n",
    "baseline = power['p_baseline']\n",
    "\n",
    "comparison = []\n",
    "for mde in mdes:\n",
    "    n = power_module.required_samples_binary(\n",
    "        p1=baseline, \n",
    "        mde=mde, \n",
    "        alpha=0.05, \n",
    "        power=0.80\n",
    "    )\n",
    "    days_at_10k = n / 10000  # Assume 10K users/day\n",
    "    comparison.append({\n",
    "        'MDE (Relative)': f'{mde:.1%}',\n",
    "        'Sample Needed': f'{n:,}',\n",
    "        'Days @ 10K/day': f'{days_at_10k:.1f}'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "display(comparison_df)\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHT: Smaller effects require exponentially more sample!\")\n",
    "print(f\"   - Detecting 1% effect needs {comparison_df.loc[0, 'Sample Needed']} users\")\n",
    "print(f\"   - Detecting 10% effect needs {comparison_df.loc[3, 'Sample Needed']} users\")\n",
    "print(f\"   - That's {int(n / power_module.required_samples_binary(baseline, 0.10, 0.05, 0.80)):d}x difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Primary Statistical Test (Z-Test for Proportions)\n",
    "\n",
    "#### üìö What is a Z-Test?\n",
    "\n",
    "**Z-test for proportions** compares two binary outcomes (control vs treatment).\n",
    "\n",
    "**Null Hypothesis (H‚ÇÄ)**: No difference between groups (p_control = p_treatment)\n",
    "\n",
    "**Alternative Hypothesis (H‚ÇÅ)**: Difference exists (p_control ‚â† p_treatment)\n",
    "\n",
    "**Output**:\n",
    "- **P-value**: Probability of seeing this result if null hypothesis is true\n",
    "- **Confidence Interval**: Range where true effect likely lies (95% CI)\n",
    "- **Effect Size**: Magnitude of difference (absolute and relative lift)\n",
    "\n",
    "**Decision Rule**: If p < 0.05, reject null hypothesis (effect is statistically significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 3: PRIMARY STATISTICAL TEST (Z-Test for Proportions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test = results['primary_test']\n",
    "\n",
    "print(f\"\\nüìä Observed Rates:\")\n",
    "print(f\"   Control conversion: {test['p_control']:.4%} ({test['x_control']}/{test['n_control']:,})\")\n",
    "print(f\"   Treatment conversion: {test['p_treatment']:.4%} ({test['x_treatment']}/{test['n_treatment']:,})\")\n",
    "\n",
    "print(f\"\\nüìä Effect Size:\")\n",
    "print(f\"   Absolute lift: {test['absolute_lift']:.4%} ({test['absolute_lift']*100:.2f} percentage points)\")\n",
    "print(f\"   Relative lift: {test['relative_lift']:.2%}\")\n",
    "\n",
    "print(f\"\\nüìä Statistical Test Results:\")\n",
    "print(f\"   Z-statistic: {test['z_stat']:.4f}\")\n",
    "print(f\"   P-value: {test['p_value']:.6f}\")\n",
    "print(f\"   Standard error: {test['se']:.6f}\")\n",
    "print(f\"   95% Confidence Interval: [{test['ci_lower']:.4%}, {test['ci_upper']:.4%}]\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETATION:\")\n",
    "print(f\"   - Null hypothesis (H‚ÇÄ): No difference between control and treatment\")\n",
    "print(f\"   - P-value = probability of seeing this result if H‚ÇÄ is true\")\n",
    "print(f\"   - Alpha = 0.05 (our threshold for rejecting H‚ÇÄ)\")\n",
    "\n",
    "if test['significant']:\n",
    "    print(f\"\\n   ‚úÖ STATISTICALLY SIGNIFICANT (p = {test['p_value']:.6f} < 0.05)\")\n",
    "    print(f\"   ‚úÖ We reject null hypothesis with 95% confidence\")\n",
    "    print(f\"   ‚úÖ Treatment shows {test['relative_lift']:.2%} lift over control\")\n",
    "    print(f\"\\n   üè¢ BUSINESS MEANING:\")\n",
    "    print(f\"      - For every 1,000 users, expect {test['absolute_lift']*1000:.1f} more conversions\")\n",
    "    print(f\"      - If 100K users/month, that's {test['absolute_lift']*100000:.0f} extra conversions/month\")\n",
    "else:\n",
    "    print(f\"\\n   ‚óã NOT SIGNIFICANT (p = {test['p_value']:.6f} ‚â• 0.05)\")\n",
    "    print(f\"   ‚óã Cannot reject null hypothesis\")\n",
    "    print(f\"   ‚óã Either: (1) no real effect OR (2) sample too small to detect it\")\n",
    "    print(f\"\\n   ü§î WHAT THIS MEANS:\")\n",
    "    print(f\"      - Observed difference ({test['relative_lift']:.2%}) could be due to random chance\")\n",
    "    print(f\"      - Consider: extending experiment or increasing traffic\")\n",
    "\n",
    "print(f\"\\nüè¢ Industry Best Practices:\")\n",
    "print(f\"   - Always report effect size + CI, not just p-value\")\n",
    "print(f\"   - P-value tells you 'is it real?', effect size tells you 'does it matter?'\")\n",
    "print(f\"   - Don't confuse statistical significance with practical importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: CUPED Variance Reduction\n",
    "\n",
    "#### üìö What is CUPED?\n",
    "\n",
    "**CUPED** = Controlled-experiment Using Pre-Experiment Data\n",
    "\n",
    "**How It Works**:\n",
    "1. Use pre-experiment covariate (e.g., `total_ads` before experiment started)\n",
    "2. Adjust outcome metric: `Y_adjusted = Y - Œ∏ * (X_pre - E[X_pre])`\n",
    "3. Run test on adjusted metric ‚Üí tighter confidence intervals!\n",
    "\n",
    "**Why It Works**:\n",
    "- Reduces noise from user heterogeneity\n",
    "- Like \"before and after\" photos - controls for baseline differences\n",
    "- Users with high pre-experiment engagement are different from low-engagement users\n",
    "\n",
    "**Requirements**:\n",
    "1. ‚úÖ Covariate measured BEFORE randomization (unaffected by treatment)\n",
    "2. ‚úÖ Covariate correlates with outcome (r > 0.3 typically effective)\n",
    "3. ‚úÖ No bias - adjustment is mathematically unbiased\n",
    "\n",
    "**Expected Impact**: 20-40% variance reduction (Netflix, Microsoft experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: CUPED VARIANCE REDUCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cuped = results.get('cuped', {})\n",
    "\n",
    "if cuped:\n",
    "    print(f\"\\nüìä Covariate Analysis:\")\n",
    "    print(f\"   Covariate used: total_ads (pre-experiment ad exposure)\")\n",
    "    print(f\"   Correlation with outcome: {cuped.get('correlation', 'N/A'):.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Variance Reduction:\")\n",
    "    print(f\"   Original variance: {cuped.get('var_original', 'N/A'):.6f}\")\n",
    "    print(f\"   Adjusted variance: {cuped.get('var_adjusted', 'N/A'):.6f}\")\n",
    "    print(f\"   Variance reduction: {cuped.get('var_reduction', 0):.2%}\")\n",
    "    print(f\"   SE reduction: {cuped.get('se_reduction', 0):.2%}\")\n",
    "    \n",
    "    print(f\"\\nüìä Test Results Comparison:\")\n",
    "    print(f\"   Original p-value: {test['p_value']:.6f}\")\n",
    "    print(f\"   CUPED-adjusted p-value: {cuped.get('p_value_adjusted', 'N/A'):.6f}\")\n",
    "    print(f\"   Original SE: {test['se']:.6f}\")\n",
    "    print(f\"   CUPED-adjusted SE: {cuped.get('se_adjusted', 'N/A'):.6f}\")\n",
    "    \n",
    "    var_red = cuped.get('var_reduction', 0)\n",
    "    print(f\"\\nüí° INTERPRETATION:\")\n",
    "    if var_red > 0.30:\n",
    "        print(f\"   ‚úÖ STRONG variance reduction ({var_red:.1%})\")\n",
    "        print(f\"   ‚úÖ CUPED very effective - covariate explains {var_red:.1%} of variance\")\n",
    "    elif var_red > 0.10:\n",
    "        print(f\"   ‚úÖ MODERATE variance reduction ({var_red:.1%})\")\n",
    "        print(f\"   ‚úÖ CUPED effective - worth using\")\n",
    "    else:\n",
    "        print(f\"   ‚óã WEAK variance reduction ({var_red:.1%})\")\n",
    "        print(f\"   ‚óã Covariate doesn't strongly predict outcome\")\n",
    "    \n",
    "    print(f\"\\n   üéØ PRACTICAL IMPACT:\")\n",
    "    sample_equiv = 1 / (1 - var_red)\n",
    "    print(f\"      - Equivalent to running experiment with {sample_equiv:.1f}x more users\")\n",
    "    print(f\"      - Or equivalently: run experiment {(1-var_red):.1%} as long for same power\")\n",
    "    print(f\"      - Example: 4-week experiment ‚Üí {4*(1-var_red):.1f} weeks with CUPED\")\n",
    "    \n",
    "    print(f\"\\nüè¢ Industry Practice:\")\n",
    "    print(f\"   - Netflix: Uses CUPED on all experiments, typically 20-40% variance reduction\")\n",
    "    print(f\"   - Microsoft: Increased experiment velocity 30% with variance reduction\")\n",
    "    print(f\"   - DoorDash: Uses CUPAC (ML-enhanced CUPED) for 30-60% reduction\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è CUPED results not available\")\n",
    "    print(\"(May require pre-experiment covariate data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Comparison Table: With vs Without CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuped:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Standard Error',\n",
    "            'P-value',\n",
    "            'Confidence Interval Width',\n",
    "            'Equivalent Sample Size',\n",
    "            'Experiment Duration'\n",
    "        ],\n",
    "        'Without CUPED': [\n",
    "            f\"{test['se']:.6f}\",\n",
    "            f\"{test['p_value']:.6f}\",\n",
    "            f\"{(test['ci_upper'] - test['ci_lower']):.4%}\",\n",
    "            \"100% (baseline)\",\n",
    "            \"4 weeks (baseline)\"\n",
    "        ],\n",
    "        'With CUPED': [\n",
    "            f\"{cuped.get('se_adjusted', 0):.6f}\",\n",
    "            f\"{cuped.get('p_value_adjusted', 0):.6f}\",\n",
    "            f\"{(cuped.get('ci_upper', 0) - cuped.get('ci_lower', 0)):.4%}\",\n",
    "            f\"{(1/(1-cuped.get('var_reduction', 0)))*100:.0f}%\",\n",
    "            f\"{4*(1-cuped.get('var_reduction', 0)):.1f} weeks\"\n",
    "        ],\n",
    "        'Improvement': [\n",
    "            f\"{cuped.get('se_reduction', 0):.1%} ‚Üì\",\n",
    "            \"Lower (more significant)\" if cuped.get('p_value_adjusted', 1) < test['p_value'] else \"Higher\",\n",
    "            f\"{cuped.get('se_reduction', 0):.1%} ‚Üì\",\n",
    "            f\"+{(1/(1-cuped.get('var_reduction', 0)) - 1)*100:.0f}%\",\n",
    "            f\"{cuped.get('var_reduction', 0):.0%} faster\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    display(comparison)\n",
    "    \n",
    "    print(f\"\\nüí° Key Takeaway: CUPED reduces variance by {cuped.get('var_reduction', 0):.1%}, \")\n",
    "    print(f\"    which is equivalent to increasing sample size by {(1/(1-cuped.get('var_reduction', 0)) - 1)*100:.0f}%!\")\n",
    "else:\n",
    "    print(\"Comparison table not available (CUPED results missing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Final Decision Summary\n",
    "\n",
    "Let's synthesize all the analysis into a clear ship/hold/abandon decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üéØ FINAL DECISION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "decision = results.get('decision', {})\n",
    "\n",
    "print(f\"\\n1. PRIMARY METRIC: {'‚úÖ SIGNIFICANT' if test['significant'] else '‚ùå NOT SIGNIFICANT'}\")\n",
    "print(f\"   - Relative lift: {test['relative_lift']:.2%}\")\n",
    "print(f\"   - P-value: {test['p_value']:.6f}\")\n",
    "print(f\"   - 95% CI: [{test['ci_lower']:.4%}, {test['ci_upper']:.4%}]\")\n",
    "\n",
    "print(f\"\\n2. RANDOMIZATION CHECK: {'‚úÖ PASSED' if not srm['srm_detected'] else '‚ùå FAILED'}\")\n",
    "print(f\"   - SRM p-value: {srm['p_value']:.6f}\")\n",
    "\n",
    "print(f\"\\n3. STATISTICAL POWER: {'‚úÖ ADEQUATE' if ratio >= 1.0 else '‚ö†Ô∏è UNDERPOWERED'}\")\n",
    "print(f\"   - Current sample: {power['current_sample']:,} per group\")\n",
    "print(f\"   - Required sample: {power['sample_per_group']:,} per group\")\n",
    "\n",
    "if cuped:\n",
    "    print(f\"\\n4. VARIANCE REDUCTION:\")\n",
    "    print(f\"   - CUPED variance reduction: {cuped.get('var_reduction', 0):.1%}\")\n",
    "    print(f\"   - Equivalent sample size gain: +{(1/(1-cuped.get('var_reduction', 0)) - 1)*100:.0f}%\")\n",
    "\n",
    "if decision:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\">>> FINAL DECISION: {decision.get('decision', 'N/A').upper()} <<<\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüìù RATIONALE:\")\n",
    "    print(f\"   {decision.get('rationale', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nüìã NEXT STEPS:\")\n",
    "    print(f\"   {decision.get('recommendation', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Decision framework results not available\")\n",
    "\n",
    "# Business impact\n",
    "business = results.get('business_impact', {})\n",
    "if business:\n",
    "    print(f\"\\nüí∞ BUSINESS IMPACT (if shipped):\")\n",
    "    print(f\"   - Incremental conversions/month: {business.get('incremental_conversions_monthly', 0):,.0f}\")\n",
    "    print(f\"   - Incremental revenue/month: ${business.get('incremental_revenue_monthly', 0):,.2f}\")\n",
    "    print(f\"   - Incremental revenue/year: ${business.get('incremental_revenue_annual', 0):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Key Learning Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Always check SRM first** - If randomization failed, nothing else matters. Don't analyze results.\n",
    "\n",
    "2. **Power analysis is essential** - Run it BEFORE starting experiments to avoid wasting time on underpowered tests.\n",
    "\n",
    "3. **CUPED is your friend** - 20-40% variance reduction = 25-67% faster experiments. Huge productivity win!\n",
    "\n",
    "4. **P-values aren't everything** - Always report effect size + confidence intervals. \"Statistically significant\" doesn't mean \"practically important\".\n",
    "\n",
    "5. **Think like a business** - Translate statistical results to dollars. Executives care about revenue impact, not p-values.\n",
    "\n",
    "### üî¨ Try These Experiments\n",
    "\n",
    "1. **Change MDE**: Run power analysis with different MDEs (1%, 5%, 10%) - how does sample size change?\n",
    "\n",
    "2. **Remove CUPED**: Compare results with and without variance reduction - is it worth the complexity?\n",
    "\n",
    "3. **Different Covariate**: Try using other pre-experiment features for CUPED - which works best?\n",
    "\n",
    "4. **Sample Size**: Run analysis on different sample fractions (0.1, 0.5, 1.0) - when do results stabilize?\n",
    "\n",
    "5. **Sensitivity Analysis**: What if conversion rate was 1% lower? How would that change the decision?\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "**Academic Papers**:\n",
    "- Deng et al. (2013): \"Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data\" (CUPED original paper)\n",
    "- Kohavi et al. (2020): \"Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing\"\n",
    "\n",
    "**Industry Blogs**:\n",
    "- [Netflix: Experimentation Platform](https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15)\n",
    "- [Booking.com: How We Measure Success](https://booking.design/how-booking-com-measures-success-57e3e33c1b5)\n",
    "- [Spotify: Confidence](https://engineering.atspotify.com/2020/03/confidence-spotify-s-tool-for-faster-experimentation-analysis/)\n",
    "\n",
    "**Next Steps**:\n",
    "- Try the Cookie Cats notebook (multiple testing correction)\n",
    "- Study the Criteo notebook (ML-enhanced techniques)\n",
    "- Read the README technique selection guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
