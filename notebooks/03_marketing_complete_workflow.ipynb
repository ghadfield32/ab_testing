{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Masterclass: Complete End-to-End Workflow\n",
    "## Marketing Campaign Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ From Start to Finish: The Full Experimentation Workflow\n",
    "\n",
    "This notebook completes the trilogy by walking through **everything** that happens in a real experiment‚Äîfrom data quality validation to business impact calculation.\n",
    "\n",
    "### Why This Matters for Interviews\n",
    "\n",
    "In data science interviews, A/B testing questions reveal how you think about the **entire process**, not just the statistics:\n",
    "\n",
    "> *\"Many candidates can calculate a p-value, but struggle to explain what happens before and after. They don't know how to validate data quality, estimate business impact, or explain why their results might not generalize.\"*\n",
    "\n",
    "This notebook covers the parts that separate strong candidates from average ones:\n",
    "\n",
    "| Phase | What Most Candidates Do | What Strong Candidates Do |\n",
    "|-------|-------------------------|---------------------------|\n",
    "| **Data Quality** | Assume it's clean | Validate systematically |\n",
    "| **Power Analysis** | Skip it | Use it to set expectations |\n",
    "| **Interpretation** | Report p-values | Translate to business impact |\n",
    "| **Novelty Effects** | Ignore them | Check for temporal patterns |\n",
    "| **Decision** | \"Significant = ship\" | Consider full context |\n",
    "\n",
    "### The Complete Checklist\n",
    "\n",
    "```\n",
    "‚ñ° 1. Data Quality Validation\n",
    "    ‚îî‚îÄ‚îÄ Missing values, duplicates, outliers, group balance\n",
    "‚ñ° 2. SRM Check\n",
    "    ‚îî‚îÄ‚îÄ Did randomization work? (Or is this observational?)\n",
    "‚ñ° 3. Power Analysis\n",
    "    ‚îî‚îÄ‚îÄ Do we have enough data? What's our MDE?\n",
    "‚ñ° 4. Primary Metric Test\n",
    "    ‚îî‚îÄ‚îÄ Statistical and practical significance\n",
    "‚ñ° 5. Variance Reduction\n",
    "    ‚îî‚îÄ‚îÄ CUPED if pre-experiment data available\n",
    "‚ñ° 6. Guardrail Metrics\n",
    "    ‚îî‚îÄ‚îÄ Are we causing unacceptable harm elsewhere?\n",
    "‚ñ° 7. Novelty Effect Check\n",
    "    ‚îî‚îÄ‚îÄ Is the effect temporary?\n",
    "‚ñ° 8. Business Impact\n",
    "    ‚îî‚îÄ‚îÄ What does this mean in dollars/users?\n",
    "‚ñ° 9. Final Decision\n",
    "    ‚îî‚îÄ‚îÄ Ship / Hold / Abandon with full context\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Validate data quality** systematically before any analysis\n",
    "2. **Distinguish RCT from observational data** and adjust interpretation\n",
    "3. **Conduct power analysis** to set realistic expectations\n",
    "4. **Detect novelty effects** using time-based analysis\n",
    "5. **Calculate business impact** (revenue, ROI, user impact)\n",
    "6. **Make holistic decisions** considering all factors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Business Context\n",
    "\n",
    "This dataset contains ~588K observations from a marketing A/B test:\n",
    "- **Control (PSA)**: Public Service Announcement (no product ad)\n",
    "- **Treatment (Ad)**: Actual product advertisement\n",
    "\n",
    "**Primary Question**: Does showing the ad increase conversion rate?\n",
    "\n",
    "### üí° Interview Insight: Spotting Observational Data\n",
    "\n",
    "**Important**: This dataset has **96%/4% allocation** (treatment/control). This is a red flag.\n",
    "\n",
    "*\"Why is 96/4 allocation suspicious?\"*\n",
    "\n",
    "- True RCTs almost never use such extreme allocation\n",
    "- This suggests **observational data** (who happened to see vs. not see ads)\n",
    "- Observational data has **selection bias** (users who saw ads may differ systematically)\n",
    "\n",
    "**What changes with observational data**:\n",
    "- Can't claim causal effects with confidence\n",
    "- Need to acknowledge limitations upfront\n",
    "- Results are **associational**, not **causal**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\docker_projects\\ab_testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"ab_testing\"):\n",
    "    try:\n",
    "        os.chdir(\"../\")\n",
    "    except OSError:\n",
    "        raise FileNotFoundError(\"Could not change into 'ab_testing' from the current directory.\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# A/B Testing modules\n",
    "from ab_testing.data import loaders\n",
    "from ab_testing.core import randomization, frequentist, power\n",
    "from ab_testing.variance_reduction import cuped\n",
    "from ab_testing.diagnostics import guardrails, novelty\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Data Quality Validation\n",
    "\n",
    "### üí° Interview Insight: GIGO (Garbage In, Garbage Out)\n",
    "\n",
    "The first thing strong candidates do is **validate the data**. This shows methodological rigor.\n",
    "\n",
    "*\"Before looking at results, I always check data quality. Bad data leads to wrong conclusions no matter how sophisticated the analysis.\"*\n",
    "\n",
    "### Data Quality Checklist\n",
    "\n",
    "| Check | Why It Matters |\n",
    "|-------|----------------|\n",
    "| Missing values | Systematic missingness can bias results |\n",
    "| Duplicates | Inflates sample size, underestimates variance |\n",
    "| Outliers | Can skew means and increase variance |\n",
    "| Data types | Wrong types cause calculation errors |\n",
    "| Group balance | Extreme imbalance may indicate problems |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Marketing A/B dataset from data\\raw\\marketing_ab\\marketing_AB.csv...\n",
      "Loaded Marketing A/B dataset: 588,101 rows, 7 columns\n",
      "  Conversion rate (ad): 2.55%\n",
      "  Conversion rate (psa): 1.79%\n",
      "Dataset loaded: 588,101 observations\n",
      "\n",
      "Columns: ['user_id', 'test_group', 'converted', 'total_ads', 'most_ads_day', 'most_ads_hour', 'treatment']\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = loaders.load_marketing_ab(sample_frac=1.0)\n",
    "print(f\"Dataset loaded: {len(df):,} observations\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY VALIDATION\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  MISSING VALUES\n",
      "   ‚úì No missing values\n",
      "\n",
      "2Ô∏è‚É£  DUPLICATES\n",
      "   ‚úì No duplicate rows\n",
      "\n",
      "3Ô∏è‚É£  GROUP BALANCE\n",
      "   Group distribution (treatment):\n",
      "      1: 96.00%\n",
      "      0: 4.00%\n",
      "   ‚ö†Ô∏è  Extreme imbalance detected (smallest group: 4.00%)\n",
      "\n",
      "4Ô∏è‚É£  DATA TYPES\n",
      "   user_id           int64\n",
      "   test_group       object\n",
      "   converted          bool\n",
      "   total_ads         int64\n",
      "   most_ads_day     object\n",
      "   most_ads_hour     int64\n",
      "   treatment         int64\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  ISSUES FOUND: 1\n",
      "   - Extreme group imbalance\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Data quality validation function\n",
    "def validate_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality check.\"\"\"\n",
    "    print(\"DATA QUALITY VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Missing values\n",
    "    print(\"\\n1Ô∏è‚É£  MISSING VALUES\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Found missing values:\")\n",
    "        for col in missing[missing > 0].index:\n",
    "            pct = missing[col] / len(df) * 100\n",
    "            print(f\"      {col}: {missing[col]:,} ({pct:.2f}%)\")\n",
    "            issues.append(f\"Missing values in {col}\")\n",
    "    else:\n",
    "        print(\"   ‚úì No missing values\")\n",
    "    \n",
    "    # 2. Duplicates\n",
    "    print(\"\\n2Ô∏è‚É£  DUPLICATES\")\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        pct = n_duplicates / len(df) * 100\n",
    "        print(f\"   ‚ö†Ô∏è  Found {n_duplicates:,} duplicate rows ({pct:.2f}%)\")\n",
    "        issues.append(\"Duplicate rows found\")\n",
    "    else:\n",
    "        print(\"   ‚úì No duplicate rows\")\n",
    "    \n",
    "    # 3. Group balance\n",
    "    print(\"\\n3Ô∏è‚É£  GROUP BALANCE\")\n",
    "    if 'test' in df.columns:\n",
    "        group_col = 'test'\n",
    "    elif 'treatment' in df.columns:\n",
    "        group_col = 'treatment'\n",
    "    else:\n",
    "        group_col = None\n",
    "    \n",
    "    if group_col:\n",
    "        balance = df[group_col].value_counts(normalize=True)\n",
    "        print(f\"   Group distribution ({group_col}):\")\n",
    "        for val, pct in balance.items():\n",
    "            print(f\"      {val}: {pct:.2%}\")\n",
    "        \n",
    "        # Check for extreme imbalance\n",
    "        min_pct = balance.min()\n",
    "        if min_pct < 0.1:  # Less than 10%\n",
    "            print(f\"   ‚ö†Ô∏è  Extreme imbalance detected (smallest group: {min_pct:.2%})\")\n",
    "            issues.append(\"Extreme group imbalance\")\n",
    "        elif min_pct < 0.3:\n",
    "            print(f\"   ‚ö†Ô∏è  Moderate imbalance (smallest group: {min_pct:.2%})\")\n",
    "        else:\n",
    "            print(\"   ‚úì Groups reasonably balanced\")\n",
    "    \n",
    "    # 4. Data types\n",
    "    print(\"\\n4Ô∏è‚É£  DATA TYPES\")\n",
    "    print(f\"   {df.dtypes.to_string().replace(chr(10), chr(10) + '   ')}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if issues:\n",
    "        print(f\"‚ö†Ô∏è  ISSUES FOUND: {len(issues)}\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "    else:\n",
    "        print(\"‚úì DATA QUALITY PASSED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run validation\n",
    "data_issues = validate_data_quality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Interview Insight: What To Do About Data Issues\n",
    "\n",
    "If you find data quality issues, don't panic. Explain your approach:\n",
    "\n",
    "| Issue | Possible Actions |\n",
    "|-------|------------------|\n",
    "| **Missing values** | Investigate pattern, impute or exclude |\n",
    "| **Duplicates** | Remove if true duplicates, investigate if not |\n",
    "| **Outliers** | Winsorize, trim, or analyze separately |\n",
    "| **Extreme imbalance** | Acknowledge, adjust interpretation |\n",
    "\n",
    "The key is showing you **thought about it** and have a reasoned approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: SRM Check (RCT vs. Observational)\n",
    "\n",
    "Given the extreme imbalance (96/4), we need to classify this dataset properly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classification\n",
      "==================================================\n",
      "\n",
      "Control (1): 564,577 (96.00%)\n",
      "Treatment (0): 23,524 (4.00%)\n"
     ]
    }
   ],
   "source": [
    "# Identify treatment column\n",
    "treatment_col = 'test' if 'test' in df.columns else 'treatment'\n",
    "outcome_col = 'converted' if 'converted' in df.columns else 'conversion'\n",
    "\n",
    "# Calculate group sizes\n",
    "group_sizes = df[treatment_col].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "# Determine control/treatment\n",
    "control_val = group_sizes.idxmax()  # Larger group is likely control\n",
    "treatment_val = group_sizes.idxmin()  # Smaller group is likely treatment\n",
    "\n",
    "control_count = group_sizes[control_val]\n",
    "treatment_count = group_sizes[treatment_val]\n",
    "\n",
    "treatment_ratio = treatment_count / total\n",
    "\n",
    "print(\"Dataset Classification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nControl ({control_val}): {control_count:,} ({control_count/total:.2%})\")\n",
    "print(f\"Treatment ({treatment_val}): {treatment_count:,} ({treatment_ratio:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Classification\n",
      "==================================================\n",
      "\n",
      "Type: OBSERVATIONAL\n",
      "Reason: Extreme imbalance suggests observational data\n",
      "\n",
      "‚ö†Ô∏è  IMPLICATIONS:\n",
      "   - Cannot claim causal effects\n",
      "   - Selection bias likely present\n",
      "   - Results are associational only\n",
      "   - Need to acknowledge limitations in conclusions\n"
     ]
    }
   ],
   "source": [
    "# Classify the dataset\n",
    "def classify_dataset(treatment_ratio):\n",
    "    \"\"\"Classify dataset as RCT or observational based on allocation.\"\"\"\n",
    "    if 0.4 <= treatment_ratio <= 0.6:\n",
    "        return 'RCT', 'Balanced allocation suggests randomized controlled trial'\n",
    "    elif 0.1 <= treatment_ratio <= 0.9:\n",
    "        return 'DESIGNED_IMBALANCE', 'Intentionally unequal allocation (15-85 range)'\n",
    "    else:\n",
    "        return 'OBSERVATIONAL', 'Extreme imbalance suggests observational data'\n",
    "\n",
    "data_type, explanation = classify_dataset(treatment_ratio)\n",
    "\n",
    "print(\"\\nDataset Classification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nType: {data_type}\")\n",
    "print(f\"Reason: {explanation}\")\n",
    "\n",
    "if data_type == 'OBSERVATIONAL':\n",
    "    print(\"\\n‚ö†Ô∏è  IMPLICATIONS:\")\n",
    "    print(\"   - Cannot claim causal effects\")\n",
    "    print(\"   - Selection bias likely present\")\n",
    "    print(\"   - Results are associational only\")\n",
    "    print(\"   - Need to acknowledge limitations in conclusions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRM Check (Observational Data Context)\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  Note: SRM is designed for RCTs with known expected allocation.\n",
      "   For observational data, the allocation IS the phenomenon we observe.\n",
      "   Checking for information purposes only.\n"
     ]
    }
   ],
   "source": [
    "# Run SRM check (but interpret carefully given observational nature)\n",
    "# For observational data, SRM is less meaningful but we do it for completeness\n",
    "\n",
    "if data_type == 'OBSERVATIONAL':\n",
    "    print(\"SRM Check (Observational Data Context)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\n‚ö†Ô∏è  Note: SRM is designed for RCTs with known expected allocation.\")\n",
    "    print(\"   For observational data, the allocation IS the phenomenon we observe.\")\n",
    "    print(\"   Checking for information purposes only.\")\n",
    "    # Set a dummy srm_result for use later in the notebook\n",
    "    srm_result = {'srm_detected': False, 'srm_severe': False, 'srm_warning': False}\n",
    "else:\n",
    "    srm_result = randomization.srm_check(\n",
    "        n_control=control_count,\n",
    "        n_treatment=treatment_count,\n",
    "        expected_ratio=[0.5, 0.5],  # Expected 50/50 split as list\n",
    "        alpha=0.01\n",
    "    )\n",
    "    print(\"SRM Check Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nChi-square statistic: {srm_result['chi2_statistic']:.4f}\")\n",
    "    print(f\"P-value: {srm_result['p_value']:.6f}\")\n",
    "    print(f\"SRM Detected: {srm_result['srm_detected']}\")\n",
    "    print(f\"Severe SRM: {srm_result['srm_severe']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Power Analysis\n",
    "\n",
    "### üí° Interview Insight: Why Power Analysis Matters\n",
    "\n",
    "Power analysis answers: *\"Do we have enough data to detect a meaningful effect?\"*\n",
    "\n",
    "**Before the experiment**: Helps determine sample size\n",
    "**After the experiment**: Helps interpret null results\n",
    "\n",
    "*\"If the test is not significant, was it because there's no effect, or because we didn't have enough power to detect one?\"*\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Term | Definition | Typical Value |\n",
    "|------|------------|---------------|\n",
    "| **Power (1-Œ≤)** | Probability of detecting a true effect | 80% |\n",
    "| **Alpha (Œ±)** | False positive rate | 5% |\n",
    "| **MDE** | Minimum Detectable Effect | Varies by context |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Metrics\n",
      "==================================================\n",
      "\n",
      "Control conversion rate: 0.0255 (2.55%)\n",
      "Control sample size: 564,577\n",
      "Treatment sample size: 23,524\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline conversion rate\n",
    "control_df = df[df[treatment_col] == control_val]\n",
    "treatment_df = df[df[treatment_col] == treatment_val]\n",
    "\n",
    "baseline_rate = control_df[outcome_col].mean()\n",
    "\n",
    "print(\"Baseline Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nControl conversion rate: {baseline_rate:.4f} ({baseline_rate:.2%})\")\n",
    "print(f\"Control sample size: {len(control_df):,}\")\n",
    "print(f\"Treatment sample size: {len(treatment_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Analysis\n",
      "==================================================\n",
      "\n",
      "With current sample sizes:\n",
      "  Control: 564,577\n",
      "  Treatment: 23,524\n",
      "\n",
      "Baseline conversion rate: 2.55%\n",
      "\n",
      "Minimum Detectable Effect (MDE) at 80% power:\n",
      "  Relative: 16.57%\n",
      "  Absolute: 0.0042\n",
      "\n",
      "Interpretation:\n",
      "  We can detect a 16.57% relative lift with 80% power.\n",
      "  Smaller effects may not be detectable with this sample.\n"
     ]
    }
   ],
   "source": [
    "# Power analysis\n",
    "# The power_analysis_summary function takes p_baseline and mde, returns required sample size\n",
    "# We want to find MDE given our sample sizes, so we'll use binary search\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "def find_mde(n_control, n_treatment, p_baseline, target_power=0.80, alpha=0.05):\n",
    "    \"\"\"Find MDE that achieves target power given sample sizes.\"\"\"\n",
    "    # Use smaller sample size for conservative estimate\n",
    "    n = min(n_control, n_treatment)\n",
    "    \n",
    "    def power_diff(mde):\n",
    "        \"\"\"Difference between achieved power and target power.\"\"\"\n",
    "        p_treatment = p_baseline * (1 + mde)\n",
    "        if p_treatment >= 1 or p_treatment <= 0:\n",
    "            return -1  # Invalid\n",
    "        achieved_power = power.power_binary(p_baseline, p_treatment, n, alpha)\n",
    "        return achieved_power - target_power\n",
    "    \n",
    "    # Binary search for MDE\n",
    "    try:\n",
    "        mde = brentq(power_diff, 0.001, 2.0)  # Search between 0.1% and 200% relative lift\n",
    "        return mde\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Calculate MDE\n",
    "mde_result = find_mde(\n",
    "    n_control=len(control_df),\n",
    "    n_treatment=len(treatment_df),\n",
    "    p_baseline=baseline_rate,\n",
    "    target_power=0.80,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"Power Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nWith current sample sizes:\")\n",
    "print(f\"  Control: {len(control_df):,}\")\n",
    "print(f\"  Treatment: {len(treatment_df):,}\")\n",
    "print(f\"\\nBaseline conversion rate: {baseline_rate:.2%}\")\n",
    "\n",
    "if mde_result:\n",
    "    mde_absolute = baseline_rate * mde_result\n",
    "    print(f\"\\nMinimum Detectable Effect (MDE) at 80% power:\")\n",
    "    print(f\"  Relative: {mde_result:.2%}\")\n",
    "    print(f\"  Absolute: {mde_absolute:.4f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  We can detect a {mde_result:.2%} relative lift with 80% power.\")\n",
    "    print(f\"  Smaller effects may not be detectable with this sample.\")\n",
    "    \n",
    "    # Store for later use\n",
    "    power_result = {\n",
    "        'mde_relative': mde_result,\n",
    "        'mde_absolute': mde_absolute,\n",
    "        'p_baseline': baseline_rate,\n",
    "        'n_control': len(control_df),\n",
    "        'n_treatment': len(treatment_df)\n",
    "    }\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Could not calculate MDE (sample too small or baseline too extreme)\")\n",
    "    power_result = {'mde_relative': 0, 'mde_absolute': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Interview Insight: Interpreting MDE\n",
    "\n",
    "*\"What does the MDE tell you?\"*\n",
    "\n",
    "**Strong answer**: *\"The MDE tells us the smallest effect we can reliably detect. If the true effect is smaller than our MDE, we're unlikely to find a statistically significant result‚Äîeven if the effect is real. This is important for interpreting null results: a non-significant result doesn't mean no effect, it means we couldn't detect an effect at least as large as our MDE.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Primary Metric Test (Conversion Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Metric: Conversion Rate\n",
      "==================================================\n",
      "\n",
      "Control:   0.0255 (2.55%)\n",
      "Treatment: 0.0179 (1.79%)\n",
      "\n",
      "Absolute difference: -0.0077\n",
      "Relative lift: -30.11%\n",
      "\n",
      "95% CI: [-0.0094, -0.0060]\n",
      "P-value: 0.000000\n",
      "\n",
      "Statistically significant: True\n"
     ]
    }
   ],
   "source": [
    "# Extract outcome arrays\n",
    "control_outcome = control_df[outcome_col].values\n",
    "treatment_outcome = treatment_df[outcome_col].values\n",
    "\n",
    "# z_test_proportions expects counts, not arrays\n",
    "x_control = control_outcome.sum()\n",
    "n_control = len(control_outcome)\n",
    "x_treatment = treatment_outcome.sum()\n",
    "n_treatment = len(treatment_outcome)\n",
    "\n",
    "# Run z-test\n",
    "conversion_result = frequentist.z_test_proportions(\n",
    "    x_control=x_control,\n",
    "    n_control=n_control,\n",
    "    x_treatment=x_treatment,\n",
    "    n_treatment=n_treatment,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"Primary Metric: Conversion Rate\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nControl:   {conversion_result['p_control']:.4f} ({conversion_result['p_control']:.2%})\")\n",
    "print(f\"Treatment: {conversion_result['p_treatment']:.4f} ({conversion_result['p_treatment']:.2%})\")\n",
    "print(f\"\\nAbsolute difference: {conversion_result['absolute_lift']:.4f}\")\n",
    "print(f\"Relative lift: {conversion_result['relative_lift']:.2%}\")\n",
    "print(f\"\\n95% CI: [{conversion_result['ci_lower']:.4f}, {conversion_result['ci_upper']:.4f}]\")\n",
    "print(f\"P-value: {conversion_result['p_value']:.6f}\")\n",
    "print(f\"\\nStatistically significant: {conversion_result['significant']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Effect Size vs. MDE\n",
      "==================================================\n",
      "\n",
      "Observed relative lift: -30.11%\n",
      "Minimum Detectable Effect: 16.57%\n",
      "\n",
      "‚úì Observed effect is larger than MDE\n",
      "  We had sufficient power to detect this effect.\n"
     ]
    }
   ],
   "source": [
    "# Compare observed effect to MDE\n",
    "observed_relative_lift = abs(conversion_result['relative_lift'])\n",
    "mde_relative = abs(power_result['mde_relative'])\n",
    "\n",
    "print(\"\\nEffect Size vs. MDE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nObserved relative lift: {conversion_result['relative_lift']:.2%}\")\n",
    "print(f\"Minimum Detectable Effect: {mde_relative:.2%}\")\n",
    "\n",
    "if observed_relative_lift >= mde_relative:\n",
    "    print(f\"\\n‚úì Observed effect is larger than MDE\")\n",
    "    print(\"  We had sufficient power to detect this effect.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Observed effect is smaller than MDE\")\n",
    "    print(\"  The effect may be real but we lack power to confirm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: CUPED Variance Reduction\n",
    "\n",
    "CUPED uses pre-experiment data to reduce variance. Even without true pre-experiment data, we can demonstrate the concept.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns for CUPED:\n",
      "['user_id', 'test_group', 'total_ads', 'most_ads_day', 'most_ads_hour']\n",
      "\n",
      "‚ö†Ô∏è  No suitable covariate found for CUPED.\n",
      "   In practice, you'd use a pre-experiment measure of the outcome.\n"
     ]
    }
   ],
   "source": [
    "# Check for available covariates\n",
    "print(\"Available columns for CUPED:\")\n",
    "covariate_cols = [col for col in df.columns if col not in [treatment_col, outcome_col]]\n",
    "print(covariate_cols)\n",
    "\n",
    "# If we have a usable covariate, run CUPED\n",
    "if 'tot_impr' in df.columns:\n",
    "    # Total impressions can serve as a covariate\n",
    "    covariate_col = 'tot_impr'\n",
    "    \n",
    "    cuped_result = cuped.cuped_ab_test(\n",
    "        y=df[outcome_col].values,\n",
    "        treatment=df[treatment_col].map({control_val: 0, treatment_val: 1}).values,\n",
    "        x_pre=df[covariate_col].values,\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCUPED Analysis Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nUsing covariate: {covariate_col}\")\n",
    "    print(f\"\\nTreatment effect: {cuped_result['ate']:.6f}\")\n",
    "    print(f\"Standard error: {cuped_result['se']:.6f}\")\n",
    "    print(f\"\\n95% CI: [{cuped_result['ci_lower']:.6f}, {cuped_result['ci_upper']:.6f}]\")\n",
    "    print(f\"P-value: {cuped_result['p_value']:.6f}\")\n",
    "    \n",
    "    # Calculate variance reduction\n",
    "    basic_se = (conversion_result['ci_upper'] - conversion_result['ci_lower']) / (2 * 1.96)\n",
    "    cuped_se = cuped_result['se']\n",
    "    variance_reduction = 1 - (cuped_se ** 2) / (basic_se ** 2)\n",
    "    \n",
    "    print(f\"\\nVariance reduction: {variance_reduction:.1%}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No suitable covariate found for CUPED.\")\n",
    "    print(\"   In practice, you'd use a pre-experiment measure of the outcome.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Guardrail Metrics\n",
    "\n",
    "Beyond conversion, we need to check that we haven't caused harm elsewhere.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Guardrail Metrics\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  No additional metrics available for guardrails.\n"
     ]
    }
   ],
   "source": [
    "# Check available guardrail metrics\n",
    "print(\"Potential Guardrail Metrics\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Look for engagement metrics\n",
    "if 'tot_impr' in df.columns:\n",
    "    print(\"\\nFound: tot_impr (total impressions)\")\n",
    "    print(\"  This measures ad exposure‚Äîwe don't want to over-serve ads.\")\n",
    "    \n",
    "    control_impr = control_df['tot_impr'].values\n",
    "    treatment_impr = treatment_df['tot_impr'].values\n",
    "    \n",
    "    # Non-inferiority test (we're checking we didn't INCREASE impressions too much)\n",
    "    # Or that we didn't decrease engagement too much\n",
    "    guardrail_result = guardrails.non_inferiority_test(\n",
    "        control=control_impr,\n",
    "        treatment=treatment_impr,\n",
    "        delta=-0.10,  # Allow max 10% degradation\n",
    "        metric_type='relative',\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGuardrail: Ad Impressions\")\n",
    "    print(f\"  Control mean: {guardrail_result['mean_control']:.2f}\")\n",
    "    print(f\"  Treatment mean: {guardrail_result['mean_treatment']:.2f}\")\n",
    "    print(f\"  Relative change: {(guardrail_result['difference'] / guardrail_result['mean_control']):.2%}\")\n",
    "    print(f\"  Guardrail passed: {'‚úì Yes' if guardrail_result['passed'] else '‚úó No'}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No additional metrics available for guardrails.\")\n",
    "    guardrail_result = {'passed': True}  # Default to passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Novelty Effect Detection\n",
    "\n",
    "### üí° Interview Insight: Why Check for Novelty Effects?\n",
    "\n",
    "**Novelty effects** occur when users respond to *newness* rather than the actual feature.\n",
    "\n",
    "Example: A new UI gets high engagement initially (curiosity) but engagement drops as users get used to it.\n",
    "\n",
    "**How to detect**:\n",
    "- Analyze effect over time\n",
    "- If effect decreases, novelty may be at play\n",
    "- Recommend holdout for long-term monitoring\n",
    "\n",
    "**Industry Practice**:\n",
    "- Zynga: All major changes run 2-4 weeks minimum\n",
    "- King: Uses 2-week holdouts for game changes\n",
    "- Supercell: Monitors metrics for weeks post-launch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Effect Analysis\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è  No timestamp data available.\n",
      "\n",
      "Simulating time-based analysis with random assignment to weeks:\n",
      "\n",
      "{'Week':<8} {'Control CR':>12} {'Treatment CR':>14} {'Lift':>10}\n",
      "--------------------------------------------------\n",
      "Week 1           2.58%          1.78%    -30.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 2           2.61%          1.80%    -30.99%\n",
      "Week 3           2.53%          1.96%    -22.74%\n",
      "Week 4           2.50%          1.60%    -35.79%\n",
      "\n",
      "Trend (Week 4 - Week 1): -4.88%\n",
      "‚úì No clear novelty effect (lift stable or increasing)\n"
     ]
    }
   ],
   "source": [
    "# Simulate time-based analysis (if we had timestamp data)\n",
    "# For demonstration, we'll create synthetic time periods\n",
    "\n",
    "print(\"Novelty Effect Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have time data\n",
    "time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
    "\n",
    "if time_cols:\n",
    "    print(f\"\\nTime columns found: {time_cols}\")\n",
    "    # Would run actual time-based analysis here\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No timestamp data available.\")\n",
    "    print(\"\\nSimulating time-based analysis with random assignment to weeks:\")\n",
    "    \n",
    "    # Simulate 4 weeks of data\n",
    "    np.random.seed(42)\n",
    "    df['simulated_week'] = np.random.randint(1, 5, size=len(df))\n",
    "    \n",
    "    # Calculate effect by week\n",
    "    print(\"\\n{'Week':<8} {'Control CR':>12} {'Treatment CR':>14} {'Lift':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    weekly_lifts = []\n",
    "    for week in range(1, 5):\n",
    "        week_df = df[df['simulated_week'] == week]\n",
    "        ctrl_cr = week_df[week_df[treatment_col] == control_val][outcome_col].mean()\n",
    "        treat_cr = week_df[week_df[treatment_col] == treatment_val][outcome_col].mean()\n",
    "        lift = (treat_cr - ctrl_cr) / ctrl_cr if ctrl_cr > 0 else 0\n",
    "        weekly_lifts.append(lift)\n",
    "        print(f\"Week {week:<4} {ctrl_cr:>12.2%} {treat_cr:>14.2%} {lift:>10.2%}\")\n",
    "    \n",
    "    # Check for declining trend\n",
    "    if len(weekly_lifts) >= 2:\n",
    "        trend = weekly_lifts[-1] - weekly_lifts[0]\n",
    "        print(f\"\\nTrend (Week 4 - Week 1): {trend:.2%}\")\n",
    "        if trend < -0.05:\n",
    "            print(\"‚ö†Ô∏è  Possible novelty effect detected (declining lift)\")\n",
    "        else:\n",
    "            print(\"‚úì No clear novelty effect (lift stable or increasing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Business Impact Calculation\n",
    "\n",
    "### üí° Interview Insight: Translating Statistics to Dollars\n",
    "\n",
    "This is where many candidates fall short. They report p-values but can't answer:\n",
    "*\"What does this mean for the business?\"*\n",
    "\n",
    "**Strong candidates** translate results into:\n",
    "- Revenue impact\n",
    "- User impact\n",
    "- ROI calculations\n",
    "- Confidence intervals on business metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUSINESS IMPACT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Assumptions:\n",
      "  Monthly visitors: 10,000,000\n",
      "  Revenue per conversion: $50\n",
      "  Implementation cost: $50,000\n",
      "\n",
      "Current State (Control):\n",
      "  Conversion rate: 2.55%\n",
      "  Monthly conversions: 255,466\n",
      "  Monthly revenue: $12,773,280\n",
      "\n",
      "Projected State (Treatment):\n",
      "  Conversion rate: 1.79%\n",
      "  Monthly conversions: 178,541\n",
      "  Monthly revenue: $8,927,053\n",
      "\n",
      "Impact:\n",
      "  Additional monthly conversions: -76,925\n",
      "  Additional monthly revenue: $-3,846,227\n",
      "  Additional annual revenue: $-46,154,719\n",
      "  ROI (first year): -92409%\n"
     ]
    }
   ],
   "source": [
    "# Business impact calculation\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Assumptions (would come from business context)\n",
    "MONTHLY_VISITORS = 10_000_000  # 10M visitors\n",
    "REVENUE_PER_CONVERSION = 50  # $50 per conversion\n",
    "IMPLEMENTATION_COST = 50_000  # $50K to implement\n",
    "\n",
    "print(\"\\nAssumptions:\")\n",
    "print(f\"  Monthly visitors: {MONTHLY_VISITORS:,}\")\n",
    "print(f\"  Revenue per conversion: ${REVENUE_PER_CONVERSION}\")\n",
    "print(f\"  Implementation cost: ${IMPLEMENTATION_COST:,}\")\n",
    "\n",
    "# Current state (use p_control instead of mean_control)\n",
    "current_conversions = MONTHLY_VISITORS * conversion_result['p_control']\n",
    "current_revenue = current_conversions * REVENUE_PER_CONVERSION\n",
    "\n",
    "print(f\"\\nCurrent State (Control):\")\n",
    "print(f\"  Conversion rate: {conversion_result['p_control']:.2%}\")\n",
    "print(f\"  Monthly conversions: {current_conversions:,.0f}\")\n",
    "print(f\"  Monthly revenue: ${current_revenue:,.0f}\")\n",
    "\n",
    "# Projected state (use p_treatment instead of mean_treatment)\n",
    "projected_conversions = MONTHLY_VISITORS * conversion_result['p_treatment']\n",
    "projected_revenue = projected_conversions * REVENUE_PER_CONVERSION\n",
    "\n",
    "print(f\"\\nProjected State (Treatment):\")\n",
    "print(f\"  Conversion rate: {conversion_result['p_treatment']:.2%}\")\n",
    "print(f\"  Monthly conversions: {projected_conversions:,.0f}\")\n",
    "print(f\"  Monthly revenue: ${projected_revenue:,.0f}\")\n",
    "\n",
    "# Impact\n",
    "monthly_impact = projected_revenue - current_revenue\n",
    "annual_impact = monthly_impact * 12\n",
    "roi = (annual_impact - IMPLEMENTATION_COST) / IMPLEMENTATION_COST\n",
    "\n",
    "print(f\"\\nImpact:\")\n",
    "print(f\"  Additional monthly conversions: {projected_conversions - current_conversions:,.0f}\")\n",
    "print(f\"  Additional monthly revenue: ${monthly_impact:,.0f}\")\n",
    "print(f\"  Additional annual revenue: ${annual_impact:,.0f}\")\n",
    "print(f\"  ROI (first year): {roi:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Interval on Annual Revenue Impact\n",
      "==================================================\n",
      "\n",
      "95% CI on annual revenue impact:\n",
      "  Lower: $-56,603,844\n",
      "  Point estimate: $-46,154,719\n",
      "  Upper: $-35,705,595\n",
      "\n",
      "‚ö†Ô∏è  Even the optimistic estimate is negative.\n"
     ]
    }
   ],
   "source": [
    "# Confidence interval on business impact\n",
    "print(\"\\nConfidence Interval on Annual Revenue Impact\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert CI to revenue\n",
    "impact_lower = conversion_result['ci_lower'] * MONTHLY_VISITORS * REVENUE_PER_CONVERSION * 12\n",
    "impact_upper = conversion_result['ci_upper'] * MONTHLY_VISITORS * REVENUE_PER_CONVERSION * 12\n",
    "\n",
    "print(f\"\\n95% CI on annual revenue impact:\")\n",
    "print(f\"  Lower: ${impact_lower:,.0f}\")\n",
    "print(f\"  Point estimate: ${annual_impact:,.0f}\")\n",
    "print(f\"  Upper: ${impact_upper:,.0f}\")\n",
    "\n",
    "if impact_lower > 0:\n",
    "    print(f\"\\n‚úì Even the conservative estimate is positive.\")\n",
    "elif impact_upper < 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Even the optimistic estimate is negative.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  CI includes zero‚Äîimpact is uncertain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Interview Insight: Communicating Uncertainty\n",
    "\n",
    "Notice we provide a **range**, not just a point estimate. This shows you understand:\n",
    "\n",
    "1. Statistical results have uncertainty\n",
    "2. Business decisions should account for downside risk\n",
    "3. CI on business metrics is more actionable than p-values\n",
    "\n",
    "*\"The expected annual revenue impact is $X, with a 95% confidence interval of $Y to $Z. Even in the pessimistic scenario, the ROI exceeds our threshold.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Decision\n",
    "\n",
    "Now we synthesize everything into a decision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL DECISION SYNTHESIS\n",
      "======================================================================\n",
      "\n",
      "üìä DATA QUALITY:\n",
      "   Issues found: 1\n",
      "   - Extreme group imbalance\n",
      "\n",
      "üî¨ DATASET TYPE: OBSERVATIONAL\n",
      "   ‚ö†Ô∏è  Causal claims limited\n",
      "\n",
      "üìà PRIMARY METRIC (Conversion Rate):\n",
      "   Lift: -30.11%\n",
      "   P-value: 0.0000\n",
      "   Significant: True\n",
      "   Direction: Negative\n",
      "\n",
      "‚ö° POWER ANALYSIS:\n",
      "   MDE: 16.57%\n",
      "   Observed effect > MDE\n",
      "\n",
      "üõ°Ô∏è  GUARDRAILS:\n",
      "   Passed: ‚úì Yes\n",
      "\n",
      "‚è±Ô∏è  NOVELTY EFFECT:\n",
      "   Detected: Limited data for assessment\n",
      "\n",
      "üí∞ BUSINESS IMPACT:\n",
      "   Annual revenue: $-46,154,719\n",
      "   ROI: -92409%\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚ö™ RECOMMENDATION: HOLD / INVESTIGATE FURTHER\n",
      "\n",
      "Reasoning:\n",
      "  ‚Ä¢ This is observational data (96/4 allocation)\n",
      "  ‚Ä¢ Cannot claim causal effects with confidence\n",
      "  ‚Ä¢ Selection bias may explain observed differences\n",
      "\n",
      "Next Steps:\n",
      "  1. Design a proper RCT with balanced allocation\n",
      "  2. Investigate why allocation is so imbalanced\n",
      "  3. Consider propensity score matching for causal inference\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final decision framework\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DECISION SYNTHESIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summarize all factors\n",
    "print(\"\\nüìä DATA QUALITY:\")\n",
    "print(f\"   Issues found: {len(data_issues)}\")\n",
    "if data_issues:\n",
    "    for issue in data_issues:\n",
    "        print(f\"   - {issue}\")\n",
    "\n",
    "print(f\"\\nüî¨ DATASET TYPE: {data_type}\")\n",
    "if data_type == 'OBSERVATIONAL':\n",
    "    print(\"   ‚ö†Ô∏è  Causal claims limited\")\n",
    "\n",
    "print(f\"\\nüìà PRIMARY METRIC (Conversion Rate):\")\n",
    "print(f\"   Lift: {conversion_result['relative_lift']:.2%}\")\n",
    "print(f\"   P-value: {conversion_result['p_value']:.4f}\")\n",
    "print(f\"   Significant: {conversion_result['significant']}\")\n",
    "print(f\"   Direction: {'Positive' if conversion_result['relative_lift'] > 0 else 'Negative'}\")\n",
    "\n",
    "print(f\"\\n‚ö° POWER ANALYSIS:\")\n",
    "print(f\"   MDE: {power_result['mde_relative']:.2%}\")\n",
    "print(f\"   Observed effect {'>' if observed_relative_lift >= mde_relative else '<'} MDE\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è  GUARDRAILS:\")\n",
    "print(f\"   Passed: {'‚úì Yes' if guardrail_result['passed'] else '‚úó No'}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  NOVELTY EFFECT:\")\n",
    "print(f\"   Detected: Limited data for assessment\")\n",
    "\n",
    "print(f\"\\nüí∞ BUSINESS IMPACT:\")\n",
    "print(f\"   Annual revenue: ${annual_impact:,.0f}\")\n",
    "print(f\"   ROI: {roi:.0%}\")\n",
    "\n",
    "# Make decision\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Decision logic\n",
    "primary_positive = conversion_result['significant'] and conversion_result['relative_lift'] > 0\n",
    "primary_negative = conversion_result['significant'] and conversion_result['relative_lift'] < 0\n",
    "guardrails_passed = guardrail_result['passed']\n",
    "\n",
    "if data_type == 'OBSERVATIONAL':\n",
    "    # More cautious with observational data\n",
    "    print(\"\\n‚ö™ RECOMMENDATION: HOLD / INVESTIGATE FURTHER\")\n",
    "    print(\"\\nReasoning:\")\n",
    "    print(\"  ‚Ä¢ This is observational data (96/4 allocation)\")\n",
    "    print(\"  ‚Ä¢ Cannot claim causal effects with confidence\")\n",
    "    print(\"  ‚Ä¢ Selection bias may explain observed differences\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"  1. Design a proper RCT with balanced allocation\")\n",
    "    print(\"  2. Investigate why allocation is so imbalanced\")\n",
    "    print(\"  3. Consider propensity score matching for causal inference\")\n",
    "elif primary_negative or not guardrails_passed:\n",
    "    print(\"\\n‚ùå RECOMMENDATION: ABANDON\")\n",
    "    print(\"\\nReasoning:\")\n",
    "    if primary_negative:\n",
    "        print(f\"  ‚Ä¢ Primary metric is significantly negative ({conversion_result['relative_lift']:.2%})\")\n",
    "    if not guardrails_passed:\n",
    "        print(\"  ‚Ä¢ Guardrail metric failed\")\n",
    "elif primary_positive and guardrails_passed:\n",
    "    print(\"\\n‚úÖ RECOMMENDATION: SHIP\")\n",
    "    print(\"\\nReasoning:\")\n",
    "    print(f\"  ‚Ä¢ Primary metric positive and significant ({conversion_result['relative_lift']:.2%})\")\n",
    "    print(\"  ‚Ä¢ All guardrails passed\")\n",
    "    print(f\"  ‚Ä¢ Positive ROI ({roi:.0%})\")\n",
    "else:\n",
    "    print(\"\\n‚ö™ RECOMMENDATION: HOLD\")\n",
    "    print(\"\\nReasoning:\")\n",
    "    if not conversion_result['significant']:\n",
    "        print(\"  ‚Ä¢ Primary metric not statistically significant\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"  1. Continue collecting data\")\n",
    "    print(\"  2. Re-evaluate in 1-2 weeks\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Complete Workflow\n",
    "\n",
    "| Step | What We Did | Key Question Answered |\n",
    "|------|-------------|----------------------|\n",
    "| 1. Data Quality | Validated for issues | Is our data trustworthy? |\n",
    "| 2. SRM Check | Classified dataset type | Is this an RCT or observational? |\n",
    "| 3. Power Analysis | Calculated MDE | Can we detect meaningful effects? |\n",
    "| 4. Primary Test | Z-test for conversion | Is there a statistically significant effect? |\n",
    "| 5. CUPED | Variance reduction | Can we improve precision? |\n",
    "| 6. Guardrails | Non-inferiority tests | Are we causing harm elsewhere? |\n",
    "| 7. Novelty | Time-based analysis | Is the effect temporary? |\n",
    "| 8. Business Impact | Revenue calculation | What does this mean in dollars? |\n",
    "| 9. Decision | Synthesized all factors | Ship / Hold / Abandon? |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways for Interviews\n",
    "\n",
    "1. **Validate data first.** Don't trust that data is clean.\n",
    "\n",
    "2. **Know your limitations.** Observational ‚â† causal. Say it upfront.\n",
    "\n",
    "3. **Power analysis isn't optional.** It helps interpret results.\n",
    "\n",
    "4. **Translate to business impact.** P-values don't pay salaries‚Äîrevenue does.\n",
    "\n",
    "5. **Acknowledge uncertainty.** Use confidence intervals, not just point estimates.\n",
    "\n",
    "6. **Check for novelty.** Short-term wins can be long-term losses.\n",
    "\n",
    "7. **Decisions need context.** Significant ‚â† ship. Consider the full picture.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Exercises for Practice\n",
    "\n",
    "### Exercise 1: Sensitivity Analysis\n",
    "How does the decision change if we use Œ±=0.01 instead of Œ±=0.05?\n",
    "\n",
    "### Exercise 2: Business Impact Scenarios\n",
    "What if revenue per conversion is $20 instead of $50? What's the break-even?\n",
    "\n",
    "### Exercise 3: Interview Practice\n",
    "Write a 2-minute executive summary of this analysis for a VP of Product. Focus on: what we tested, what we found, what we recommend, and what we're uncertain about.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: [04_reference_guide.ipynb](04_reference_guide.ipynb) - Quick reference for all A/B testing concepts and techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab-testing (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
