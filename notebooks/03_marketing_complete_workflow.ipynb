{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Masterclass: Complete End-to-End Workflow\n",
    "## Marketing Campaign Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Data Quality Validation** - Why and how to validate your data first\n",
    "2. **SRM Detection** - Identifying randomization failures\n",
    "3. **Power Analysis** - Determining if you have enough data\n",
    "4. **CUPED Variance Reduction** - Using pre-experiment data for efficiency\n",
    "5. **Guardrail Metrics** - Protecting critical metrics\n",
    "6. **Novelty Effect Detection** - Identifying temporary effects\n",
    "7. **Business Impact** - Translating statistics to dollars\n",
    "8. **Decision Framework** - Ship/Hold/Abandon logic\n",
    "\n",
    "---\n",
    "\n",
    "### The Business Context\n",
    "\n",
    "This dataset contains ~588K observations from a marketing A/B test comparing:\n",
    "- **Control (PSA)**: Public Service Announcement (no product ad)\n",
    "- **Treatment (Ad)**: Actual product advertisement\n",
    "\n",
    "**Primary Question:** Does showing the ad increase conversion rate?\n",
    "\n",
    "**Important Note:** This dataset has 96%/4% allocation (treatment/control), which suggests it may be **observational data** rather than a true randomized experiment. We'll address this in the analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\docker_projects\\ab_testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"ab_testing\"):\n",
    "    try:\n",
    "        os.chdir(\"../\")\n",
    "    except OSError:\n",
    "        raise FileNotFoundError(\"Could not change into 'ab_testing' from the current directory.\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any\n",
    "\n",
    "# A/B Testing modules\n",
    "from ab_testing.data import loaders\n",
    "from ab_testing.core import randomization, frequentist, power\n",
    "from ab_testing.variance_reduction import cuped\n",
    "from ab_testing.diagnostics import guardrails, novelty\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load and Validate Data\n",
    "\n",
    "### Why Data Quality Matters\n",
    "\n",
    "**Garbage In, Garbage Out (GIGO)**\n",
    "\n",
    "Common data quality issues that invalidate experiments:\n",
    "- Missing values (especially systematic)\n",
    "- Duplicates (inflates sample size)\n",
    "- Outliers (skew results)\n",
    "- Data type errors\n",
    "- Group imbalance\n",
    "\n",
    "**Always validate BEFORE running any analysis.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Marketing A/B dataset (sample=0.1)...\n",
      "Loading Marketing A/B dataset from data\\raw\\marketing_ab\\marketing_AB.csv...\n",
      "Loaded Marketing A/B dataset: 588,101 rows, 7 columns\n",
      "  Conversion rate (ad): 2.55%\n",
      "  Conversion rate (psa): 1.79%\n",
      "  Sampled to 58,810 rows (10.0% of full dataset)\n",
      "\n",
      "‚úì Loaded 58,810 observations\n",
      "\n",
      "Columns: ['user_id', 'test_group', 'converted', 'total_ads', 'most_ads_day', 'most_ads_hour', 'treatment']\n",
      "\n",
      "Data types:\n",
      "user_id           int64\n",
      "test_group       object\n",
      "converted          bool\n",
      "total_ads         int64\n",
      "most_ads_day     object\n",
      "most_ads_hour     int64\n",
      "treatment         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load data with 10% sample (for learning speed)\n",
    "SAMPLE_FRAC = 0.1\n",
    "\n",
    "print(f\"Loading Marketing A/B dataset (sample={SAMPLE_FRAC})...\")\n",
    "df = loaders.load_marketing_ab(sample_frac=SAMPLE_FRAC)\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(df):,} observations\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Validation:\n",
      "==================================================\n",
      "\n",
      "1. Missing Values:\n",
      "   ‚úì No missing values\n",
      "\n",
      "2. Duplicates:\n",
      "   ‚úì No duplicate rows\n",
      "\n",
      "3. Group Distribution:\n",
      "   {'ad': 56424, 'psa': 2386}\n",
      "\n",
      "4. Outcome Variable (converted):\n",
      "   Unique values: [False  True]\n",
      "   Type: bool\n"
     ]
    }
   ],
   "source": [
    "# Data quality checks\n",
    "print(\"Data Quality Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Missing values\n",
    "print(f\"\\n1. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(f\"   ‚úì No missing values\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Missing values found:\")\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "# 2. Duplicates\n",
    "print(f\"\\n2. Duplicates:\")\n",
    "n_duplicates = df.duplicated().sum()\n",
    "if n_duplicates == 0:\n",
    "    print(f\"   ‚úì No duplicate rows\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  {n_duplicates:,} duplicate rows found\")\n",
    "\n",
    "# 3. Group distribution\n",
    "print(f\"\\n3. Group Distribution:\")\n",
    "group_counts = df['test_group'].value_counts()\n",
    "print(f\"   {group_counts.to_dict()}\")\n",
    "\n",
    "# 4. Outcome variable\n",
    "print(f\"\\n4. Outcome Variable (converted):\")\n",
    "print(f\"   Unique values: {df['converted'].unique()}\")\n",
    "print(f\"   Type: {df['converted'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Summary:\n",
      "==================================================\n",
      "Control (PSA):   2,386 observations\n",
      "Treatment (Ad):  56,424 observations\n",
      "Total:           58,810 observations\n",
      "\n",
      "Allocation ratio (treatment/control): 23.65\n",
      "\n",
      "‚ö†Ô∏è  SEVERE IMBALANCE: This is likely OBSERVATIONAL data\n",
      "   True experiments typically have ratios near 1.0\n",
      "   Causal claims should be made with caution\n"
     ]
    }
   ],
   "source": [
    "# Separate groups\n",
    "control = df[df['test_group'] == 'psa']\n",
    "treatment = df[df['test_group'] == 'ad']\n",
    "\n",
    "print(\"\\nGroup Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Control (PSA):   {len(control):,} observations\")\n",
    "print(f\"Treatment (Ad):  {len(treatment):,} observations\")\n",
    "print(f\"Total:           {len(df):,} observations\")\n",
    "\n",
    "# Allocation ratio\n",
    "ratio = len(treatment) / len(control)\n",
    "print(f\"\\nAllocation ratio (treatment/control): {ratio:.2f}\")\n",
    "\n",
    "if ratio > 10:\n",
    "    print(f\"\\n‚ö†Ô∏è  SEVERE IMBALANCE: This is likely OBSERVATIONAL data\")\n",
    "    print(f\"   True experiments typically have ratios near 1.0\")\n",
    "    print(f\"   Causal claims should be made with caution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Sample Ratio Mismatch (SRM) Check\n",
    "\n",
    "### Understanding This Dataset\n",
    "\n",
    "This dataset has **~96%/4% allocation** (treatment/control). This is unusual for a true A/B test and suggests:\n",
    "\n",
    "1. **Observational data** - Users self-selected into groups\n",
    "2. **Designed imbalance** - Intentional (for cost/risk reasons)\n",
    "3. **Data labeling error** - Mislabeled as \"A/B test\"\n",
    "\n",
    "**Important:** For this analysis, we'll treat this as **observational data**, which means:\n",
    "- We can measure **association**, not **causation**\n",
    "- Confounding variables may bias results\n",
    "- Propensity score matching would improve causal inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Classification:\n",
      "==================================================\n",
      "\n",
      "Type: OBSERVATIONAL DATA (not a true RCT)\n",
      "\n",
      "Reason: 23.6x imbalance suggests self-selection\n",
      "        True experiments typically have 1:1 to 4:1 ratios\n",
      "\n",
      "Implications:\n",
      "  ‚Ä¢ Results show ASSOCIATION, not CAUSATION\n",
      "  ‚Ä¢ Confounding variables may bias estimates\n",
      "  ‚Ä¢ Use for learning, but interpret with caution\n"
     ]
    }
   ],
   "source": [
    "# Calculate observed allocation\n",
    "n_control = len(control)\n",
    "n_treatment = len(treatment)\n",
    "total = n_control + n_treatment\n",
    "\n",
    "observed_ratio_control = n_control / total\n",
    "observed_ratio_treatment = n_treatment / total\n",
    "\n",
    "# This dataset is OBSERVATIONAL with ~4%/96% split\n",
    "# We'll check if sample matches this expected pattern\n",
    "IS_RCT = False  # NOT a true randomized controlled trial\n",
    "EXPECTED_ALLOCATION = [0.04, 0.96]  # Observed baseline pattern\n",
    "\n",
    "print(\"Dataset Classification:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nType: OBSERVATIONAL DATA (not a true RCT)\")\n",
    "print(f\"\\nReason: {n_treatment/n_control:.1f}x imbalance suggests self-selection\")\n",
    "print(f\"        True experiments typically have 1:1 to 4:1 ratios\")\n",
    "print(f\"\\nImplications:\")\n",
    "print(f\"  ‚Ä¢ Results show ASSOCIATION, not CAUSATION\")\n",
    "print(f\"  ‚Ä¢ Confounding variables may bias estimates\")\n",
    "print(f\"  ‚Ä¢ Use for learning, but interpret with caution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Allocation Check (Diagnostic):\n",
      "==================================================\n",
      "Expected (baseline pattern): 4.0% / 96.0%\n",
      "Observed (this sample):      4.06% / 95.94%\n",
      "\n",
      "Chi-square: 0.4999\n",
      "P-value:    0.479537\n",
      "\n",
      "‚úì Sample matches expected pattern\n"
     ]
    }
   ],
   "source": [
    "# Run allocation check (diagnostic only for observational data)\n",
    "srm_result = randomization.srm_check(\n",
    "    n_control=n_control,\n",
    "    n_treatment=n_treatment,\n",
    "    expected_ratio=EXPECTED_ALLOCATION,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "print(\"\\nAllocation Check (Diagnostic):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Expected (baseline pattern): {EXPECTED_ALLOCATION[0]:.1%} / {EXPECTED_ALLOCATION[1]:.1%}\")\n",
    "print(f\"Observed (this sample):      {observed_ratio_control:.2%} / {observed_ratio_treatment:.2%}\")\n",
    "print(f\"\\nChi-square: {srm_result['chi2_statistic']:.4f}\")\n",
    "print(f\"P-value:    {srm_result['p_value']:.6f}\")\n",
    "\n",
    "if not srm_result['srm_detected']:\n",
    "    print(f\"\\n‚úì Sample matches expected pattern\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Sample differs from expected pattern\")\n",
    "    print(f\"   Check data loading/filtering logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Power Analysis\n",
    "\n",
    "### Why Power Analysis Matters\n",
    "\n",
    "**Power** = Probability of detecting a real effect when it exists (1 - Œ≤)\n",
    "\n",
    "**MDE** = Minimum Detectable Effect (smallest change we can reliably detect)\n",
    "\n",
    "| Power | Interpretation |\n",
    "|-------|---------------|\n",
    "| 80% | Industry standard - 20% chance of missing real effect |\n",
    "| 90% | Conservative - 10% chance of missing real effect |\n",
    "| 95% | Very conservative - 5% chance of missing real effect |\n",
    "\n",
    "### The Formula\n",
    "\n",
    "For binary outcomes (conversion), required sample per group:\n",
    "\n",
    "$$n = \\frac{2(z_{\\alpha/2} + z_\\beta)^2 \\cdot p(1-p)}{\\delta^2}$$\n",
    "\n",
    "Where:\n",
    "- $z_{\\alpha/2}$ = 1.96 for Œ±=0.05 (two-sided)\n",
    "- $z_\\beta$ = 0.84 for 80% power\n",
    "- $p$ = baseline conversion rate\n",
    "- $\\delta$ = MDE (absolute difference)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Rates:\n",
      "==================================================\n",
      "Control (PSA):   0.0189 (1.89%)\n",
      "Treatment (Ad):  0.0256 (2.56%)\n",
      "\n",
      "Observed relative lift: 35.69%\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline conversion rate\n",
    "p_control = control['converted'].mean()\n",
    "p_treatment = treatment['converted'].mean()\n",
    "observed_lift = (p_treatment / p_control - 1) * 100\n",
    "\n",
    "print(\"Conversion Rates:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Control (PSA):   {p_control:.4f} ({p_control*100:.2f}%)\")\n",
    "print(f\"Treatment (Ad):  {p_treatment:.4f} ({p_treatment*100:.2f}%)\")\n",
    "print(f\"\\nObserved relative lift: {observed_lift:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Analysis:\n",
      "==================================================\n",
      "\n",
      "Inputs:\n",
      "  Baseline conversion: 1.89%\n",
      "  Target MDE:          2.0% relative\n",
      "  Alpha (Type I):      0.05 (5%)\n",
      "  Power (1 - Type II): 0.80 (80%)\n",
      "\n",
      "Results:\n",
      "  Treatment rate with MDE: 1.92%\n",
      "  Required sample per group: 2,061,546\n",
      "  Total required: 4,123,092\n",
      "  Effect size (Cohen's h): 0.0028\n"
     ]
    }
   ],
   "source": [
    "# Run power analysis\n",
    "TARGET_MDE = 0.02  # 2% relative MDE (e.g., 2% ‚Üí 2.04%)\n",
    "\n",
    "power_result = power.power_analysis_summary(\n",
    "    p_baseline=p_control,\n",
    "    mde=TARGET_MDE,  # 2% relative lift\n",
    "    alpha=0.05,\n",
    "    power=0.80\n",
    ")\n",
    "\n",
    "print(\"Power Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInputs:\")\n",
    "print(f\"  Baseline conversion: {p_control:.2%}\")\n",
    "print(f\"  Target MDE:          {TARGET_MDE:.1%} relative\")\n",
    "print(f\"  Alpha (Type I):      0.05 (5%)\")\n",
    "print(f\"  Power (1 - Type II): 0.80 (80%)\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Treatment rate with MDE: {power_result['p_treatment']:.2%}\")\n",
    "print(f\"  Required sample per group: {power_result['sample_per_group']:,}\")\n",
    "print(f\"  Total required: {power_result['sample_per_group'] * 2:,}\")\n",
    "print(f\"  Effect size (Cohen's h): {power_result['cohens_h']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Power Assessment:\n",
      "==================================================\n",
      "Required per group: 2,061,546\n",
      "Actual (smaller group): 2,386\n",
      "\n",
      "‚ö†Ô∏è  UNDERPOWERED: Only 0.1% of required sample\n",
      "  Risk: May miss real effects (false negatives)\n",
      "\n",
      "  Options:\n",
      "  1. Extend experiment to collect more data\n",
      "  2. Target larger MDE (accept coarser resolution)\n",
      "  3. Use variance reduction (CUPED/CUPAC)\n"
     ]
    }
   ],
   "source": [
    "# Compare to actual sample\n",
    "actual_per_group = min(n_control, n_treatment)\n",
    "required_per_group = power_result['sample_per_group']\n",
    "\n",
    "print(\"\\nPower Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Required per group: {required_per_group:,}\")\n",
    "print(f\"Actual (smaller group): {actual_per_group:,}\")\n",
    "\n",
    "if actual_per_group >= required_per_group:\n",
    "    ratio_achieved = actual_per_group / required_per_group\n",
    "    print(f\"\\n‚úì WELL-POWERED: {ratio_achieved:.1f}x required sample\")\n",
    "    print(f\"  Can detect effects as small as {TARGET_MDE:.1%} with 80% confidence\")\n",
    "else:\n",
    "    ratio_achieved = actual_per_group / required_per_group\n",
    "    print(f\"\\n‚ö†Ô∏è  UNDERPOWERED: Only {ratio_achieved:.1%} of required sample\")\n",
    "    print(f\"  Risk: May miss real effects (false negatives)\")\n",
    "    print(f\"\\n  Options:\")\n",
    "    print(f\"  1. Extend experiment to collect more data\")\n",
    "    print(f\"  2. Target larger MDE (accept coarser resolution)\")\n",
    "    print(f\"  3. Use variance reduction (CUPED/CUPAC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Primary Metric Test\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Metric: Conversion Rate\n",
      "==================================================\n",
      "\n",
      "Control:   0.0189 (45/2,386)\n",
      "Treatment: 0.0256 (1,444/56,424)\n",
      "\n",
      "Absolute difference: 0.0067 (0.67pp)\n",
      "Relative lift:       35.69%\n",
      "\n",
      "Z-statistic: 2.0504\n",
      "P-value:     0.040330\n",
      "95% CI:      [0.0011, 0.0123]\n",
      "\n",
      "Statistically significant: True\n"
     ]
    }
   ],
   "source": [
    "# Z-test for conversion rate\n",
    "x_control = control['converted'].sum()\n",
    "x_treatment = treatment['converted'].sum()\n",
    "\n",
    "primary_result = frequentist.z_test_proportions(\n",
    "    x_control=x_control,\n",
    "    n_control=n_control,\n",
    "    x_treatment=x_treatment,\n",
    "    n_treatment=n_treatment,\n",
    "    alpha=0.05,\n",
    "    two_sided=True\n",
    ")\n",
    "\n",
    "print(\"Primary Metric: Conversion Rate\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nControl:   {p_control:.4f} ({x_control:,}/{n_control:,})\")\n",
    "print(f\"Treatment: {p_treatment:.4f} ({x_treatment:,}/{n_treatment:,})\")\n",
    "print(f\"\\nAbsolute difference: {primary_result['absolute_lift']:.4f} ({primary_result['absolute_lift']*100:.2f}pp)\")\n",
    "print(f\"Relative lift:       {primary_result['relative_lift']:.2%}\")\n",
    "print(f\"\\nZ-statistic: {primary_result['z_statistic']:.4f}\")\n",
    "print(f\"P-value:     {primary_result['p_value']:.6f}\")\n",
    "print(f\"95% CI:      [{primary_result['ci_lower']:.4f}, {primary_result['ci_upper']:.4f}]\")\n",
    "print(f\"\\nStatistically significant: {primary_result['significant']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretation:\n",
      "==================================================\n",
      "‚úì STATISTICALLY SIGNIFICANT (p = 0.0403 < 0.05)\n",
      "\n",
      "What this means:\n",
      "  ‚Ä¢ If there were truly no effect, we'd see this result\n",
      "    only 4.03% of the time by chance\n",
      "  ‚Ä¢ Treatment shows 35.69% lift\n",
      "  ‚Ä¢ 95% confident true lift is between\n",
      "    0.11pp and 1.23pp\n",
      "\n",
      "‚ö†Ô∏è  CAUTION (Observational Data):\n",
      "  ‚Ä¢ This shows ASSOCIATION, not necessarily CAUSATION\n",
      "  ‚Ä¢ Users who saw ads may differ from PSA users\n",
      "  ‚Ä¢ Confounding variables may explain the difference\n"
     ]
    }
   ],
   "source": [
    "# Interpret the result\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if primary_result['significant']:\n",
    "    print(f\"‚úì STATISTICALLY SIGNIFICANT (p = {primary_result['p_value']:.4f} < 0.05)\")\n",
    "    print(f\"\\nWhat this means:\")\n",
    "    print(f\"  ‚Ä¢ If there were truly no effect, we'd see this result\")\n",
    "    print(f\"    only {primary_result['p_value']*100:.2f}% of the time by chance\")\n",
    "    print(f\"  ‚Ä¢ Treatment shows {primary_result['relative_lift']:.2%} lift\")\n",
    "    print(f\"  ‚Ä¢ 95% confident true lift is between\")\n",
    "    print(f\"    {primary_result['ci_lower']*100:.2f}pp and {primary_result['ci_upper']*100:.2f}pp\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  CAUTION (Observational Data):\")\n",
    "    print(f\"  ‚Ä¢ This shows ASSOCIATION, not necessarily CAUSATION\")\n",
    "    print(f\"  ‚Ä¢ Users who saw ads may differ from PSA users\")\n",
    "    print(f\"  ‚Ä¢ Confounding variables may explain the difference\")\n",
    "else:\n",
    "    print(f\"‚óã NOT STATISTICALLY SIGNIFICANT (p = {primary_result['p_value']:.4f} ‚â• 0.05)\")\n",
    "    print(f\"\\nWhat this means:\")\n",
    "    print(f\"  ‚Ä¢ Cannot confidently say treatment differs from control\")\n",
    "    print(f\"  ‚Ä¢ Either: (1) No real effect, or (2) Sample too small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: CUPED Variance Reduction\n",
    "\n",
    "### What is CUPED?\n",
    "\n",
    "**CUPED** = Controlled-experiment Using Pre-Experiment Data\n",
    "\n",
    "It uses **pre-experiment covariates** to reduce noise in your metrics:\n",
    "\n",
    "$$Y_{\\text{adjusted}} = Y - \\theta(X - \\bar{X})$$\n",
    "\n",
    "Where:\n",
    "- $Y$ = outcome (conversion)\n",
    "- $X$ = pre-experiment covariate (must be unaffected by treatment)\n",
    "- $\\theta = \\text{Cov}(Y, X) / \\text{Var}(X)$ = optimal adjustment coefficient\n",
    "\n",
    "### Why CUPED Works\n",
    "\n",
    "If a covariate predicts the outcome, it explains some of the variance. By adjusting for this, we reduce noise and increase power.\n",
    "\n",
    "**Variance reduction** = $r^2$ (correlation squared)\n",
    "\n",
    "| Correlation | Variance Reduction | Sample Equivalent |\n",
    "|-------------|-------------------|-------------------|\n",
    "| 0.30 | 9% | 1.1x more users |\n",
    "| 0.50 | 25% | 1.33x more users |\n",
    "| 0.70 | 49% | 2x more users |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUPED Setup:\n",
      "==================================================\n",
      "Outcome: converted (binary)\n",
      "Covariate: total_ads (pre-experiment ad exposure)\n",
      "\n",
      "Covariate statistics:\n",
      "  Control mean:    24.29\n",
      "  Treatment mean:  24.86\n"
     ]
    }
   ],
   "source": [
    "# Use total_ads as pre-experiment covariate\n",
    "# (number of ads shown before outcome - proxy for user engagement)\n",
    "\n",
    "control_outcome = control['converted'].values\n",
    "control_covariate = control['total_ads'].values\n",
    "\n",
    "treatment_outcome = treatment['converted'].values\n",
    "treatment_covariate = treatment['total_ads'].values\n",
    "\n",
    "print(\"CUPED Setup:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Outcome: converted (binary)\")\n",
    "print(f\"Covariate: total_ads (pre-experiment ad exposure)\")\n",
    "print(f\"\\nCovariate statistics:\")\n",
    "print(f\"  Control mean:    {control_covariate.mean():.2f}\")\n",
    "print(f\"  Treatment mean:  {treatment_covariate.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUPED Results:\n",
      "==================================================\n",
      "\n",
      "Covariate Information:\n",
      "  Correlation with outcome: 0.2173\n",
      "  Theta (adjustment coef):  0.000570\n",
      "\n",
      "Variance Reduction:\n",
      "  Variance reduction: 4.4%\n",
      "  SE reduction:       1.4%\n",
      "  Sample size equivalent: 1.05x\n",
      "\n",
      "Adjusted Test:\n",
      "  Raw p-value:      0.040330\n",
      "  CUPED p-value:    0.017084\n",
      "  Change:           -0.023246\n"
     ]
    }
   ],
   "source": [
    "# Run CUPED\n",
    "cuped_result = cuped.cuped_ab_test(\n",
    "    y_control=control_outcome,\n",
    "    y_treatment=treatment_outcome,\n",
    "    x_control=control_covariate,\n",
    "    x_treatment=treatment_covariate,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nCUPED Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nCovariate Information:\")\n",
    "print(f\"  Correlation with outcome: {cuped_result['correlation']:.4f}\")\n",
    "print(f\"  Theta (adjustment coef):  {cuped_result['theta']:.6f}\")\n",
    "\n",
    "print(f\"\\nVariance Reduction:\")\n",
    "print(f\"  Variance reduction: {cuped_result['var_reduction']:.1%}\")\n",
    "print(f\"  SE reduction:       {cuped_result['se_reduction']:.1%}\")\n",
    "print(f\"  Sample size equivalent: {1/(1-cuped_result['var_reduction']):.2f}x\")\n",
    "\n",
    "print(f\"\\nAdjusted Test:\")\n",
    "print(f\"  Raw p-value:      {primary_result['p_value']:.6f}\")\n",
    "print(f\"  CUPED p-value:    {cuped_result['p_value_adjusted']:.6f}\")\n",
    "print(f\"  Change:           {cuped_result['p_value_adjusted'] - primary_result['p_value']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUPED Effectiveness:\n",
      "==================================================\n",
      "‚óã WEAK variance reduction (4.4%)\n",
      "  Covariate doesn't strongly predict outcome\n",
      "  Consider finding better pre-experiment predictors\n"
     ]
    }
   ],
   "source": [
    "# Assess CUPED effectiveness\n",
    "print(\"\\nCUPED Effectiveness:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cuped_result['var_reduction'] > 0.20:\n",
    "    print(f\"‚úì STRONG variance reduction ({cuped_result['var_reduction']:.1%})\")\n",
    "    print(f\"  Covariate explains {cuped_result['var_reduction']:.1%} of outcome variance\")\n",
    "    print(f\"  Like running with {1/(1-cuped_result['var_reduction']):.1f}x more users!\")\n",
    "elif cuped_result['var_reduction'] > 0.10:\n",
    "    print(f\"‚úì MODERATE variance reduction ({cuped_result['var_reduction']:.1%})\")\n",
    "    print(f\"  Helpful noise reduction\")\n",
    "elif cuped_result['var_reduction'] > 0.05:\n",
    "    print(f\"‚óã MODEST variance reduction ({cuped_result['var_reduction']:.1%})\")\n",
    "    print(f\"  Some benefit but not dramatic\")\n",
    "else:\n",
    "    print(f\"‚óã WEAK variance reduction ({cuped_result['var_reduction']:.1%})\")\n",
    "    print(f\"  Covariate doesn't strongly predict outcome\")\n",
    "    print(f\"  Consider finding better pre-experiment predictors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Guardrail Metrics\n",
    "\n",
    "### The Guardrail Framework\n",
    "\n",
    "| Metric Type | Purpose | Test Type |\n",
    "|-------------|---------|----------|\n",
    "| **Primary** | What we OPTIMIZE | Standard hypothesis test |\n",
    "| **Guardrail** | What we PROTECT | Non-inferiority test |\n",
    "\n",
    "**Non-inferiority test:**\n",
    "- Question: \"Is degradation within acceptable threshold?\"\n",
    "- NOT asking \"is it better?\" - just \"is it not too bad?\"\n",
    "- Pass if: Lower bound of CI > threshold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail: Average Ads Shown\n",
      "==================================================\n",
      "Tolerance: -5.0% (max allowed degradation)\n",
      "\n",
      "Control mean:   24.29\n",
      "Treatment mean: 24.86\n",
      "\n",
      "Relative change: 2.33%\n",
      "95% CI lower:    -0.8023\n",
      "\n",
      "Result: ‚úì PASSED\n"
     ]
    }
   ],
   "source": [
    "# Guardrail: Average ads shown should not decrease significantly\n",
    "guardrail_control = control['total_ads'].values\n",
    "guardrail_treatment = treatment['total_ads'].values\n",
    "\n",
    "guardrail_result = guardrails.non_inferiority_test(\n",
    "    control=guardrail_control,\n",
    "    treatment=guardrail_treatment,\n",
    "    delta=-0.05,  # Allow up to 5% degradation\n",
    "    metric_type='relative',\n",
    "    alpha=0.05\n",
    ")\n",
    "guardrail_result['metric_name'] = 'avg_ads_shown'\n",
    "\n",
    "print(\"Guardrail: Average Ads Shown\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Tolerance: -5.0% (max allowed degradation)\")\n",
    "print(f\"\\nControl mean:   {guardrail_result['mean_control']:.2f}\")\n",
    "print(f\"Treatment mean: {guardrail_result['mean_treatment']:.2f}\")\n",
    "\n",
    "# Calculate relative change\n",
    "rel_change = guardrail_result['difference'] / guardrail_result['mean_control']\n",
    "print(f\"\\nRelative change: {rel_change:.2%}\")\n",
    "print(f\"95% CI lower:    {guardrail_result['ci_lower']:.4f}\")\n",
    "print(f\"\\nResult: {'‚úì PASSED' if guardrail_result['passed'] else '‚úó FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Novelty Effect Detection\n",
    "\n",
    "### What are Novelty Effects?\n",
    "\n",
    "**Novelty effect** = Temporary spike due to user curiosity, not genuine value\n",
    "\n",
    "| Week | Effect | Interpretation |\n",
    "|------|--------|----------------|\n",
    "| 1 | +15% | Users exploring new feature |\n",
    "| 2 | +10% | Novelty wearing off |\n",
    "| 3 | +5% | Returning to baseline |\n",
    "| 4 | +3% | True sustained effect |\n",
    "\n",
    "**Why it matters:**\n",
    "- Shipping novelty = wasted engineering\n",
    "- Users may actually dislike feature once novelty wears off\n",
    "- Need to distinguish temporary from sustained effects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Based Analysis:\n",
      "==================================================\n",
      "\n",
      "Time periods available: 7\n",
      "\n",
      "Conversion by day (control):\n",
      "most_ads_day\n",
      "Friday       0.010782\n",
      "Monday       0.022857\n",
      "Saturday     0.019934\n",
      "Sunday       0.032468\n",
      "Thursday     0.019465\n",
      "Tuesday      0.021127\n",
      "Wednesday    0.008310\n",
      "Name: converted, dtype: float64\n",
      "\n",
      "Conversion by day (treatment):\n",
      "most_ads_day\n",
      "Friday       0.018317\n",
      "Monday       0.033400\n",
      "Saturday     0.024449\n",
      "Sunday       0.023664\n",
      "Thursday     0.023724\n",
      "Tuesday      0.030918\n",
      "Wednesday    0.025439\n",
      "Name: converted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Aggregate conversion by day of week (proxy for time)\n",
    "daily_control = control.groupby('most_ads_day')['converted'].mean().sort_index()\n",
    "daily_treatment = treatment.groupby('most_ads_day')['converted'].mean().sort_index()\n",
    "\n",
    "print(\"Time-Based Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTime periods available: {len(daily_control)}\")\n",
    "print(f\"\\nConversion by day (control):\")\n",
    "print(daily_control)\n",
    "print(f\"\\nConversion by day (treatment):\")\n",
    "print(daily_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚óã Insufficient time points (7) for novelty analysis\n",
      "   Need ‚â•10 time points for reliable detection\n",
      "   Recommendation: Run longer experiments (2-4 weeks)\n"
     ]
    }
   ],
   "source": [
    "# Check if we have enough time points for novelty analysis\n",
    "MIN_TIME_POINTS = 10\n",
    "\n",
    "if len(daily_control) >= MIN_TIME_POINTS:\n",
    "    # Run novelty detection\n",
    "    novelty_result = novelty.detect_novelty_effect(\n",
    "        metrics_control=daily_control.values,\n",
    "        metrics_treatment=daily_treatment.values,\n",
    "        window_size=3,\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    print(\"Novelty Effect Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nEarly period effect: {novelty_result['early_effect']:.4f}\")\n",
    "    print(f\"Late period effect:  {novelty_result['late_effect']:.4f}\")\n",
    "    print(f\"Effect decay:        {novelty_result['effect_decay']:.4f}\")\n",
    "    print(f\"Decay p-value:       {novelty_result['decay_pvalue']:.4f}\")\n",
    "    \n",
    "    if novelty_result['novelty_detected']:\n",
    "        print(f\"\\n‚ö†Ô∏è  NOVELTY EFFECT DETECTED!\")\n",
    "        print(f\"   Effect is WEAKENING over time\")\n",
    "        print(f\"   Recommendation: Run post-launch holdout (2-4 weeks)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì NO NOVELTY EFFECT DETECTED\")\n",
    "        print(f\"   Effect appears STABLE across time\")\n",
    "else:\n",
    "    novelty_result = None\n",
    "    print(f\"\\n‚óã Insufficient time points ({len(daily_control)}) for novelty analysis\")\n",
    "    print(f\"   Need ‚â•{MIN_TIME_POINTS} time points for reliable detection\")\n",
    "    print(f\"   Recommendation: Run longer experiments (2-4 weeks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Business Impact Calculation\n",
    "\n",
    "### From Statistics to Dollars\n",
    "\n",
    "P-values tell us \"is it real?\" but business impact tells us \"does it matter?\"\n",
    "\n",
    "**Components:**\n",
    "1. **Scale**: How many users affected?\n",
    "2. **Magnitude**: How big is the effect per user?\n",
    "3. **Value**: What's each conversion worth?\n",
    "4. **Time**: Annualized for comparisons\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{Annual Value} = \\text{Users} \\times \\text{Lift} \\times \\text{Value per Conversion} \\times 12$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Impact Calculation:\n",
      "==================================================\n",
      "\n",
      "üìä ASSUMPTIONS:\n",
      "   Monthly active users: 1,000,000\n",
      "   Average order value:  $10.00\n",
      "   Baseline conversion:  1.89%\n",
      "   Treatment conversion: 2.56%\n"
     ]
    }
   ],
   "source": [
    "# Business assumptions (customize for your business)\n",
    "MONTHLY_USERS = 1_000_000\n",
    "AVG_ORDER_VALUE = 10.0\n",
    "\n",
    "print(\"Business Impact Calculation:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä ASSUMPTIONS:\")\n",
    "print(f\"   Monthly active users: {MONTHLY_USERS:,}\")\n",
    "print(f\"   Average order value:  ${AVG_ORDER_VALUE:.2f}\")\n",
    "print(f\"   Baseline conversion:  {p_control:.2%}\")\n",
    "print(f\"   Treatment conversion: {p_treatment:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà MONTHLY IMPACT:\n",
      "   Baseline conversions:     18,860\n",
      "   Treatment conversions:    25,592\n",
      "   Incremental conversions:  6,732\n",
      "   Incremental revenue:      $67,319.30\n",
      "\n",
      "üí∞ ANNUALIZED IMPACT:\n",
      "   Best case (95% CI upper):  $1,481,219.81\n",
      "   Expected (point estimate): $807,831.59\n",
      "   Worst case (95% CI lower): $134,443.37\n"
     ]
    }
   ],
   "source": [
    "# Calculate business impact\n",
    "baseline_conversions_monthly = MONTHLY_USERS * p_control\n",
    "treatment_conversions_monthly = MONTHLY_USERS * p_treatment\n",
    "incremental_conversions = treatment_conversions_monthly - baseline_conversions_monthly\n",
    "\n",
    "incremental_revenue_monthly = incremental_conversions * AVG_ORDER_VALUE\n",
    "incremental_revenue_annual = incremental_revenue_monthly * 12\n",
    "\n",
    "# Confidence interval bounds (on the LIFT, not absolute)\n",
    "# CI is on the difference (absolute), so multiply by users and value\n",
    "worst_case_annual = MONTHLY_USERS * primary_result['ci_lower'] * AVG_ORDER_VALUE * 12\n",
    "best_case_annual = MONTHLY_USERS * primary_result['ci_upper'] * AVG_ORDER_VALUE * 12\n",
    "\n",
    "print(f\"\\nüìà MONTHLY IMPACT:\")\n",
    "print(f\"   Baseline conversions:     {baseline_conversions_monthly:,.0f}\")\n",
    "print(f\"   Treatment conversions:    {treatment_conversions_monthly:,.0f}\")\n",
    "print(f\"   Incremental conversions:  {incremental_conversions:,.0f}\")\n",
    "print(f\"   Incremental revenue:      ${incremental_revenue_monthly:,.2f}\")\n",
    "\n",
    "print(f\"\\nüí∞ ANNUALIZED IMPACT:\")\n",
    "print(f\"   Best case (95% CI upper):  ${best_case_annual:,.2f}\")\n",
    "print(f\"   Expected (point estimate): ${incremental_revenue_annual:,.2f}\")\n",
    "print(f\"   Worst case (95% CI lower): ${worst_case_annual:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COST-BENEFIT ANALYSIS:\n",
      "   Expected annual value: $807,831.59\n",
      "\n",
      "   If implementation cost = $403,916:\n",
      "      ‚Üí Break-even in ~6 months - GOOD investment\n",
      "\n",
      "   If implementation cost > $807,832:\n",
      "      ‚Üí Need multi-year value or strategic reasons\n",
      "\n",
      "‚ö†Ô∏è  Risk assessment:\n",
      "   ‚úì Even in worst case, still profitable ($134,443)\n",
      "   ‚úì Low downside risk\n"
     ]
    }
   ],
   "source": [
    "# ROI analysis\n",
    "print(f\"\\nüìä COST-BENEFIT ANALYSIS:\")\n",
    "print(f\"   Expected annual value: ${incremental_revenue_annual:,.2f}\")\n",
    "print(f\"\\n   If implementation cost = ${incremental_revenue_annual * 0.5:,.0f}:\")\n",
    "print(f\"      ‚Üí Break-even in ~6 months - GOOD investment\")\n",
    "print(f\"\\n   If implementation cost > ${incremental_revenue_annual:,.0f}:\")\n",
    "print(f\"      ‚Üí Need multi-year value or strategic reasons\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Risk assessment:\")\n",
    "if worst_case_annual > 0:\n",
    "    print(f\"   ‚úì Even in worst case, still profitable (${worst_case_annual:,.0f})\")\n",
    "    print(f\"   ‚úì Low downside risk\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Worst case is NEGATIVE (${worst_case_annual:,.0f})\")\n",
    "    print(f\"   ‚ö†Ô∏è  Consider: Is upside worth the downside risk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Final Decision\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DECISION FRAMEWORK\n",
      "============================================================\n",
      "\n",
      "üéØ Primary Metric: Conversion Rate\n",
      "   Significant: True\n",
      "   Positive:    True\n",
      "   Lift:        35.69%\n",
      "\n",
      "üõ°Ô∏è  Guardrails:\n",
      "   Passed: 1/1\n",
      "\n",
      "üí∞ Business Impact:\n",
      "   Annual value: $807,832\n",
      "\n",
      "============================================================\n",
      ">>> FINAL DECISION: SHIP <<<\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Make final decision\n",
    "decision_result = guardrails.evaluate_guardrails(\n",
    "    primary_result=primary_result,\n",
    "    guardrail_results=[guardrail_result]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DECISION FRAMEWORK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüéØ Primary Metric: Conversion Rate\")\n",
    "print(f\"   Significant: {decision_result['primary_significant']}\")\n",
    "print(f\"   Positive:    {decision_result['primary_positive']}\")\n",
    "print(f\"   Lift:        {primary_result['relative_lift']:.2%}\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è  Guardrails:\")\n",
    "print(f\"   Passed: {decision_result['guardrails_passed']}/{decision_result['guardrails_total']}\")\n",
    "\n",
    "if novelty_result is not None:\n",
    "    print(f\"\\n‚è±Ô∏è  Novelty:\")\n",
    "    if novelty_result['novelty_detected']:\n",
    "        print(f\"   ‚ö†Ô∏è  Detected - effect may be temporary\")\n",
    "    else:\n",
    "        print(f\"   ‚úì Not detected - effect appears stable\")\n",
    "\n",
    "print(f\"\\nüí∞ Business Impact:\")\n",
    "print(f\"   Annual value: ${incremental_revenue_annual:,.0f}\")\n",
    "\n",
    "decision = decision_result['decision'].upper()\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\">>> FINAL DECISION: {decision} <<<\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DECISION INTERPRETATION\n",
      "============================================================\n",
      "\n",
      "‚úÖ SHIP RECOMMENDATION\n",
      "\n",
      "Rationale:\n",
      "  ‚Ä¢ Primary metric improved significantly (35.69%)\n",
      "  ‚Ä¢ All guardrails passed\n",
      "  ‚Ä¢ Positive business impact ($807,832/year)\n",
      "\n",
      "Next steps:\n",
      "  1. Prepare rollout plan\n",
      "  2. Set up monitoring dashboards\n",
      "  3. Define rollback criteria\n"
     ]
    }
   ],
   "source": [
    "# Decision explanation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if decision == 'SHIP':\n",
    "    print(f\"\\n‚úÖ SHIP RECOMMENDATION\")\n",
    "    print(f\"\\nRationale:\")\n",
    "    print(f\"  ‚Ä¢ Primary metric improved significantly ({primary_result['relative_lift']:.2%})\")\n",
    "    print(f\"  ‚Ä¢ All guardrails passed\")\n",
    "    print(f\"  ‚Ä¢ Positive business impact (${incremental_revenue_annual:,.0f}/year)\")\n",
    "    if novelty_result is not None and novelty_result['novelty_detected']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Caveat: Novelty effect detected\")\n",
    "        print(f\"   Run 2-4 week post-launch holdout to verify sustained effect\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"  1. Prepare rollout plan\")\n",
    "    print(f\"  2. Set up monitoring dashboards\")\n",
    "    print(f\"  3. Define rollback criteria\")\n",
    "\n",
    "elif decision == 'ABANDON':\n",
    "    print(f\"\\n‚ùå ABANDON RECOMMENDATION\")\n",
    "    print(f\"\\nRationale:\")\n",
    "    if not decision_result['primary_positive']:\n",
    "        print(f\"  ‚Ä¢ Primary metric showed NEGATIVE impact\")\n",
    "    if decision_result['guardrails_passed'] < decision_result['guardrails_total']:\n",
    "        print(f\"  ‚Ä¢ Guardrail(s) FAILED\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"  1. Analyze why it failed\")\n",
    "    print(f\"  2. Generate new hypotheses\")\n",
    "    print(f\"  3. Design improved treatment\")\n",
    "\n",
    "else:  # HOLD\n",
    "    print(f\"\\n‚ö™ HOLD RECOMMENDATION\")\n",
    "    print(f\"\\nRationale:\")\n",
    "    if not decision_result['primary_significant']:\n",
    "        print(f\"  ‚Ä¢ Primary metric not statistically significant\")\n",
    "    print(f\"\\nOptions:\")\n",
    "    print(f\"  1. Extend experiment (more data)\")\n",
    "    print(f\"  2. Increase traffic allocation\")\n",
    "    print(f\"  3. Use variance reduction (CUPED/CUPAC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Complete A/B Testing Checklist\n",
    "\n",
    "### Always Do These (in order)\n",
    "\n",
    "| Step | Question | Tool |\n",
    "|------|----------|------|\n",
    "| 1 | Is data quality OK? | Manual checks |\n",
    "| 2 | Is randomization valid? | `srm_check()` |\n",
    "| 3 | Do we have enough data? | `power_analysis_summary()` |\n",
    "| 4 | Is the effect real? | `z_test_proportions()` |\n",
    "| 5 | Can we reduce noise? | `cuped_ab_test()` |\n",
    "| 6 | Did we harm anything? | `non_inferiority_test()` |\n",
    "| 7 | Is it sustainable? | `detect_novelty_effect()` |\n",
    "| 8 | Is it worth it? | Business impact calculation |\n",
    "| 9 | Ship/Hold/Abandon? | `evaluate_guardrails()` |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Peeking**: Stopping early without proper sequential testing\n",
    "2. **Ignoring SRM**: Trusting results from broken randomization\n",
    "3. **No guardrails**: Optimizing primary at expense of critical metrics\n",
    "4. **Novelty blind**: Shipping temporary effects\n",
    "5. **P-value obsession**: Significant ‚â† meaningful\n",
    "6. **No power analysis**: Running underpowered experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Try Different Sample Sizes\n",
    "\n",
    "Run the analysis with sample_frac=0.5 and sample_frac=1.0. How do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Vary Guardrail Thresholds\n",
    "\n",
    "What happens if you use a stricter guardrail threshold (-2% instead of -5%)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Business Assumptions\n",
    "\n",
    "How sensitive is the business impact to your assumptions? Try:\n",
    "- 500K users instead of 1M\n",
    "- $5 AOV instead of $10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab-testing (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
