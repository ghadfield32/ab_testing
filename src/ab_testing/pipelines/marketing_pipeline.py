"""
Marketing A/B Test Analysis Pipeline

This module demonstrates a complete end-to-end A/B testing workflow using the
Marketing A/B Testing dataset (588K observations).

Dataset: https://www.kaggle.com/datasets/faviovaz/marketing-ab-testing
Use Case: Evaluate effectiveness of marketing advertisements

Pipeline Steps:
1. Load and validate data
2. Check randomization quality (SRM)
3. Run primary metric analysis (conversion rate)
4. Apply variance reduction (CUPED/CUPAC)
5. Check guardrail metrics
6. Detect novelty effects
7. Make ship/hold/abandon decision
8. Calculate business impact
"""

import numpy as np
import pandas as pd
from typing import Dict, Any

from ab_testing.data import loaders
from ab_testing.core import randomization, frequentist, power
from ab_testing.variance_reduction import cuped
from ab_testing.diagnostics import guardrails, novelty, aa_tests
from ab_testing.decision import framework


def run_marketing_analysis(
    sample_frac: float = 0.1,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Run complete A/B test analysis on Marketing dataset.

    Parameters
    ----------
    sample_frac : float, default=0.1
        Fraction of data to use (0.0-1.0). Use smaller values for faster testing.
    verbose : bool, default=True
        Print detailed progress and results.

    Returns
    -------
    Dict[str, Any]
        Complete analysis results including:
        - data_summary: Dataset statistics
        - srm_check: Sample ratio mismatch results
        - primary_test: Main conversion test results
        - variance_reduction: CUPED/CUPAC results
        - guardrails: Guardrail metric checks
        - novelty: Time-based effect analysis
        - decision: Ship/hold/abandon recommendation
        - business_impact: Revenue/conversion projections

    Examples
    --------
    >>> results = run_marketing_analysis(sample_frac=0.01, verbose=True)
    >>> print(results['decision']['recommendation'])
    'ship'
    >>> print(results['primary_test']['p_value'])
    0.0023
    """

    results = {}

    # ========================================================================
    # STEP 1: Load and Validate Data
    # ========================================================================
    if verbose:
        print("="*70)
        print("MARKETING A/B TEST ANALYSIS PIPELINE")
        print("="*70)
        print(f"\n[1/8] Loading Marketing A/B Testing Dataset (sample={sample_frac})...")
        print("\nüìö LEARNING: Why Data Quality Matters")
        print("   - Bad data ‚Üí bad decisions (garbage in, garbage out)")
        print("   - Always validate: completeness, types, ranges, distributions")
        print("   - Real datasets have issues: missing values, outliers, duplicates")
        print("   - Check group balance BEFORE running any statistical tests")

    df = loaders.load_marketing_ab(sample_frac=sample_frac)

    results['data_summary'] = {
        'total_observations': len(df),
        'control_size': len(df[df['test_group'] == 'psa']),
        'treatment_size': len(df[df['test_group'] == 'ad']),
        'conversion_rate_control': df[df['test_group'] == 'psa']['converted'].mean(),
        'conversion_rate_treatment': df[df['test_group'] == 'ad']['converted'].mean(),
        'date_range': f"{df['most_ads_day'].min()} to {df['most_ads_day'].max()}",
        'avg_ads_shown': df['total_ads'].mean(),
    }

    if verbose:
        print(f"\n‚úì Data Quality Check:")
        print(f"   Total observations: {results['data_summary']['total_observations']:,}")
        print(f"   Control (PSA): {results['data_summary']['control_size']:,}")
        print(f"   Treatment (Ad): {results['data_summary']['treatment_size']:,}")
        print(f"   Date range: {results['data_summary']['date_range']}")
        print(f"   Conversion (control): {results['data_summary']['conversion_rate_control']:.2%}")
        print(f"   Conversion (treatment): {results['data_summary']['conversion_rate_treatment']:.2%}")

        ratio = results['data_summary']['treatment_size'] / results['data_summary']['control_size']
        print(f"\nüí° INTERPRETATION:")
        print(f"   - Group size ratio: {ratio:.3f} (should be ~1.0 for 50/50 split)")
        if abs(ratio - 1.0) > 0.05:
            print(f"   ‚ö†Ô∏è  WARNING: Significant imbalance detected!")
            print(f"   ‚ö†Ô∏è  This will be checked formally with SRM test in next step")
        else:
            print(f"   ‚úì Groups appear balanced - good start!")
        print(f"   - Observed lift: {(results['data_summary']['conversion_rate_treatment']/results['data_summary']['conversion_rate_control'] - 1):.2%}")
        print(f"   - Next: Verify randomization quality before trusting this lift")

    # ========================================================================
    # STEP 2: Check Randomization Quality (SRM)
    # ========================================================================
    if verbose:
        print(f"\n[2/8] Checking randomization quality (Sample Ratio Mismatch)...")
        print("\nüìö LEARNING: Sample Ratio Mismatch (SRM)")
        print("   - SRM = when group sizes don't match expected allocation")
        print("   - Indicates randomization FAILURE ‚Üí all results invalid")
        print("   - Common causes:")
        print("     ‚Ä¢ Implementation bugs in randomization code")
        print("     ‚Ä¢ Telemetry/tracking issues (lost events)")
        print("     ‚Ä¢ Bot traffic or fraud")
        print("     ‚Ä¢ Variant-specific bugs causing crashes")
        print("   - Test: Chi-square goodness-of-fit with alpha=0.01 (strict)")
        print("   - Industry standard: STOP experiment if SRM detected")

    control = df[df['test_group'] == 'psa']
    treatment = df[df['test_group'] == 'ad']

    # DATASET CONFIGURATION: Marketing AB is OBSERVATIONAL, not a true RCT
    # Actual allocation: 96% ad / 4% psa (severe imbalance indicates non-random assignment)
    # This appears to be user self-selection or observational data mislabeled as A/B test
    IS_RCT = False  # This is NOT a properly randomized controlled trial
    EXPECTED_ALLOCATION = [0.04, 0.96]  # Observed allocation (psa=4%, ad=96%)

    # Calculate observed allocation
    n_control = len(control)
    n_treatment = len(treatment)
    observed_ratio_control = n_control / (n_control + n_treatment)
    observed_ratio_treatment = n_treatment / (n_control + n_treatment)

    if verbose:
        print(f"\n‚ö†Ô∏è  DATASET WARNING: Observational Data (NOT RCT)")
        print(f"   This dataset has 96/4 allocation which indicates:")
        print(f"   - NOT a properly randomized A/B test")
        print(f"   - Likely observational/self-selected groups")
        print(f"   - Causal inference requires propensity score matching or similar adjustments")
        print(f"   - Results should be interpreted as CORRELATIONAL, not CAUSAL")
        print()

    # Run allocation imbalance check (diagnostic only, NOT true SRM)
    results['srm_check'] = randomization.srm_check(
        n_control=n_control,
        n_treatment=n_treatment,
        expected_ratio=EXPECTED_ALLOCATION,
        alpha=0.01
    )

    # Mark as observational in results
    results['srm_check']['is_rct'] = IS_RCT
    results['srm_check']['allocation_type'] = 'observational'

    if verbose:
        print(f"\n‚úì Allocation Imbalance Check (Diagnostic Only):")
        print(f"   Note: This is NOT a true SRM check (dataset is observational)")
        print(f"   Expected (observed baseline): {EXPECTED_ALLOCATION[0]:.1%} / {EXPECTED_ALLOCATION[1]:.1%}")
        print(f"   Actual (this run): {observed_ratio_control:.2%} / {observed_ratio_treatment:.2%}")
        print(f"   Chi-square statistic: {results['srm_check']['chi2_statistic']:.4f}")
        print(f"   P-value: {results['srm_check']['p_value']:.6f}")

        print(f"\nüí° INTERPRETATION:")
        if not results['srm_check']['srm_detected']:
            print(f"   ‚úì Allocation matches expected pattern")
            print(f"   ‚óã HOWEVER: This does NOT validate randomization (dataset is observational)")
            print(f"   ‚óã Statistical tests will show associations, not causal effects")
        else:
            print(f"   ‚ö†Ô∏è Allocation differs from expected pattern")
            print(f"   ‚ö†Ô∏è This might indicate sampling/filtering issues")
            print(f"   ‚ö†Ô∏è Check data loading and filtering logic")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Netflix: Always runs SRM check before analyzing experiments")
        print(f"   - Booking.com: Uses alpha=0.001 (even stricter) for high-stakes tests")
        print(f"   - Microsoft: Automated SRM monitoring with alerts")
        print(f"   - Best practice: Check SRM multiple times during experiment")

    # For observational data, no hard gate - proceed with analysis
    # Mark status as VALID for consistency with other pipelines
    results['status'] = 'VALID'
    results['status_reason'] = 'Observational data - allocation check diagnostic only'

    # ========================================================================
    # STEP 3: Power Analysis & Primary Metric Test
    # ========================================================================
    if verbose:
        print(f"\n[3/8] Running primary metric analysis (conversion rate)...")
        print("\nüìö LEARNING: Power Analysis & Hypothesis Testing")
        print("   Power Analysis:")
        print("   - Power = probability of detecting a real effect (1 - Œ≤)")
        print("   - MDE = Minimum Detectable Effect (smallest change we can reliably detect)")
        print("   - Sample size depends on: baseline rate, MDE, alpha, power")
        print("   - Industry standard: 80% power, 5% alpha, 10-20% relative MDE")
        print("   ")
        print("   Hypothesis Testing:")
        print("   - H0 (null): No difference between control and treatment")
        print("   - H1 (alternative): Treatment effect exists")
        print("   - P-value: Probability of seeing this result if H0 is true")
        print("   - Alpha: Threshold for rejecting H0 (typically 0.05 = 5%)")
        print("   - Type I error (Œ±): False positive (saying effect exists when it doesn't)")
        print("   - Type II error (Œ≤): False negative (missing a real effect)")

    # Power analysis
    p_control = results['data_summary']['conversion_rate_control']
    observed_lift = results['data_summary']['conversion_rate_treatment'] / p_control - 1

    results['power_analysis'] = power.power_analysis_summary(
        p_baseline=p_control,
        mde=0.02,  # 2% relative minimum detectable effect
        alpha=0.05,
        power=0.80
    )

    # Primary test
    x_control = control['converted'].sum()
    x_treatment = treatment['converted'].sum()

    results['primary_test'] = frequentist.z_test_proportions(
        x_control=x_control,
        n_control=len(control),
        x_treatment=x_treatment,
        n_treatment=len(treatment),
        alpha=0.05,
        two_sided=True
    )

    if verbose:
        print(f"\n‚úì Power Analysis (Prospective):")
        print(f"   Baseline conversion: {p_control:.2%}")
        print(f"   Target MDE: {results['power_analysis']['mde_relative']:.1%} relative")
        print(f"   Treatment (with MDE): {results['power_analysis']['p_treatment']:.2%}")
        print(f"   Required sample per group: {results['power_analysis']['sample_per_group']:,}")
        print(f"   Current sample per group: ~{len(control):,}")
        print(f"   Effect size (Cohen's h): {results['power_analysis']['cohens_h']:.4f}")

        if len(control) >= results['power_analysis']['sample_per_group']:
            print(f"   ‚úì Well-powered: {len(control)/results['power_analysis']['sample_per_group']:.1f}x required sample")
            print(f"   ‚úì Can detect effects as small as {results['power_analysis']['mde_relative']:.1%} with 80% confidence")
        else:
            print(f"   ‚ö†Ô∏è  Underpowered: Only {len(control)/results['power_analysis']['sample_per_group']:.1%} of required sample")
            print(f"   ‚ö†Ô∏è  Risk: May miss real effects (false negatives)")

        print(f"\n‚úì Statistical Test Results (Z-test for proportions):")
        print(f"   Control: {p_control:.4%} ({x_control:,} / {len(control):,})")
        print(f"   Treatment: {results['data_summary']['conversion_rate_treatment']:.4%} ({x_treatment:,} / {len(treatment):,})")
        print(f"   Absolute difference: {results['primary_test']['absolute_lift']:.4%} ({results['primary_test']['absolute_lift']*100:.2f} percentage points)")
        print(f"   Relative lift: {results['primary_test']['relative_lift']:.2%}")
        print(f"   Z-statistic: {results['primary_test']['z_statistic']:.4f}")
        print(f"   P-value (two-sided): {results['primary_test']['p_value']:.6f}")
        print(f"   95% Confidence Interval: [{results['primary_test']['ci_lower']:.4%}, {results['primary_test']['ci_upper']:.4%}]")

        print(f"\nüí° INTERPRETATION:")
        if results['primary_test']['significant']:
            print(f"   ‚úì STATISTICALLY SIGNIFICANT (p = {results['primary_test']['p_value']:.6f} < 0.05)")
            print(f"   ‚úì We reject the null hypothesis with 95% confidence")
            print(f"   ‚úì Treatment shows {results['primary_test']['relative_lift']:.2%} lift over control")
            print(f"   ")
            print(f"   What this means in plain English:")
            print(f"   - If there were truly no effect, we'd see a result this extreme")
            print(f"     only {results['primary_test']['p_value']*100:.2f}% of the time by random chance")
            print(f"   - Since {results['primary_test']['p_value']*100:.2f}% < 5%, we conclude the effect is real")
            print(f"   ")
            print(f"   Business impact:")
            print(f"   - For every 1,000 users, expect {results['primary_test']['absolute_lift']*1000:.1f} more conversions")
            print(f"   - 95% confident the true lift is between {results['primary_test']['ci_lower']*100:.2f}pp and {results['primary_test']['ci_upper']*100:.2f}pp")
        else:
            print(f"   ‚óã NOT STATISTICALLY SIGNIFICANT (p = {results['primary_test']['p_value']:.6f} ‚â• 0.05)")
            print(f"   ‚óã Cannot reject the null hypothesis")
            print(f"   ")
            print(f"   What this means:")
            print(f"   - Either: (1) No real effect exists, OR")
            print(f"   -        (2) Effect exists but sample too small to detect it")
            print(f"   - Observed {results['primary_test']['relative_lift']:.2%} lift could be random noise")
            print(f"   ")
            print(f"   Options:")
            print(f"   1. Extend experiment to collect more data")
            print(f"   2. Accept result and abandon this variant")
            print(f"   3. Redesign treatment for stronger expected effect")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Booking.com: Runs 25,000+ experiments/year, 90%+ are powered correctly")
        print(f"   - Microsoft: Minimum 7-day runtime even if significance reached early")
        print(f"   - Netflix: Uses two-sided tests (can detect both positive and negative effects)")
        print(f"   - Common mistake: Stopping experiment as soon as p < 0.05 (p-hacking)")
        print(f"   - Best practice: Pre-register target sample size and stick to it")

    # ========================================================================
    # STEP 4: Variance Reduction (CUPED)
    # ========================================================================
    if verbose:
        print(f"\n[4/8] Applying variance reduction (CUPED)...")
        print("\nüìö LEARNING: CUPED (Controlled-experiment Using Pre-Experiment Data)")
        print("   What is CUPED?")
        print("   - Uses pre-experiment data to reduce noise in experiment metrics")
        print("   - Like 'before and after' photos - controls for baseline differences")
        print("   - Can reduce sample size needed by 10-50% ‚Üí faster experiments!")
        print("   ")
        print("   How it works:")
        print("   - Find a covariate X that correlates with outcome Y")
        print("   - Must be measured BEFORE experiment (unaffected by treatment)")
        print("   - Adjust: Y_adjusted = Y - Œ∏(X - X_mean)")
        print("   - Where Œ∏ = Cov(Y,X) / Var(X) minimizes variance")
        print("   ")
        print("   Requirements for good covariate:")
        print("   ‚úì Strongly correlated with outcome (higher correlation ‚Üí more reduction)")
        print("   ‚úì Measured before experiment start (pre-period data)")
        print("   ‚úì Not affected by treatment assignment")
        print("   ")
        print("   We use 'total_ads' (pre-experiment exposure) to predict conversions")

    # Use total_ads as pre-experiment covariate
    control_outcome = control['converted'].values
    control_covariate = control['total_ads'].values

    treatment_outcome = treatment['converted'].values
    treatment_covariate = treatment['total_ads'].values

    results['cuped'] = cuped.cuped_ab_test(
        y_control=control_outcome,
        y_treatment=treatment_outcome,
        x_control=control_covariate,
        x_treatment=treatment_covariate,
        alpha=0.05
    )

    if verbose:
        print(f"\n‚úì CUPED Adjustment Results:")
        print(f"   Covariate: total_ads (pre-experiment ad exposure)")
        print(f"   Correlation with outcome: {results['cuped']['correlation']:.4f}")
        print(f"   Theta (adjustment coefficient): {results['cuped']['theta']:.6f}")
        print(f"   ")
        print(f"   Variance Metrics:")
        original_var = np.var(control_outcome, ddof=1)
        adjusted_var = original_var * (1 - results['cuped']['var_reduction'])
        print(f"   Original control variance: {original_var:.6f}")
        print(f"   Adjusted control variance: {adjusted_var:.6f}")
        print(f"   Variance reduction: {results['cuped']['var_reduction']:.2%}")
        print(f"   Standard error reduction: {results['cuped']['se_reduction']:.2%}")
        print(f"   Equivalent sample size multiplier: {1/(1-results['cuped']['var_reduction']):.2f}x")
        print(f"   ")
        print(f"   Statistical Test (CUPED-adjusted):")
        print(f"   Raw p-value: {results['primary_test']['p_value']:.6f}")
        print(f"   CUPED-adjusted p-value: {results['cuped']['p_value_adjusted']:.6f}")
        print(f"   Change: {results['cuped']['p_value_adjusted'] - results['primary_test']['p_value']:.6f}")

        print(f"\nüí° INTERPRETATION:")
        if results['cuped']['var_reduction'] > 0.20:
            print(f"   ‚úì STRONG variance reduction ({results['cuped']['var_reduction']:.1%})")
            print(f"   ‚úì CUPED very effective - covariate explains {results['cuped']['var_reduction']:.1%} of variance")
            print(f"   ‚úì This is like running the experiment with {1/(1-results['cuped']['var_reduction']):.1f}x more users!")
        elif results['cuped']['var_reduction'] > 0.10:
            print(f"   ‚úì MODERATE variance reduction ({results['cuped']['var_reduction']:.1%})")
            print(f"   ‚úì CUPED helpful - reduces noise meaningfully")
        else:
            print(f"   ‚óã WEAK variance reduction ({results['cuped']['var_reduction']:.1%})")
            print(f"   ‚óã Covariate doesn't strongly predict outcome")
            print(f"   ‚óã Consider finding better pre-experiment predictors")

        print(f"   ")
        print(f"   Practical impact:")
        if results['cuped']['var_reduction'] > 0.20:
            print(f"   - Could run future experiments {int(results['cuped']['sample_size_reduction']*100)}% shorter")
            print(f"   - Or detect {int(100 - results['cuped']['var_reduction']*100)}% smaller effects with same sample")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Netflix: Uses CUPED on all experiments, typically 20-40% variance reduction")
        print(f"   - Microsoft: Standard practice, reduced experiment runtime by 30%")
        print(f"   - Booking.com: Combines multiple covariates for even stronger reduction")
        print(f"   - Common covariates: previous behavior, user characteristics, historical metrics")
        print(f"   - Best practice: Always use CUPED when pre-period data available")

    # ========================================================================
    # STEP 5: Guardrail Metrics
    # ========================================================================
    if verbose:
        print(f"\n[5/8] Checking guardrail metrics...")
        print("\nüìö LEARNING: Guardrail Metrics Framework")
        print("   What are guardrails?")
        print("   - Primary metric: What we're trying to IMPROVE")
        print("   - Guardrails: Metrics we must NOT HARM")
        print("   - Example: Improve clicks (primary) but don't hurt revenue (guardrail)")
        print("   ")
        print("   Why guardrails matter:")
        print("   - Prevent unintended consequences (\"Goodhart's Law\")")
        print("   - Optimize one thing ‚Üí accidentally break another")
        print("   - Example: Increase engagement ‚Üí but users hate the experience")
        print("   ")
        print("   Non-inferiority test:")
        print("   - Question: 'Is degradation within acceptable threshold?'")
        print("   - NOT asking 'is it better?' - just 'is it not too much worse?'")
        print("   - Test: Lower bound of 95% CI must be > -delta")
        print("   - Common thresholds: -2% for revenue, -5% for engagement, -1% for critical metrics")
        print("   ")
        print("   Multiple testing correction:")
        print("   - DON'T apply Bonferroni to guardrails (Spotify approach)")
        print("   - Instead: Ensure high power (80%+) to detect degradation")
        print("   - Conservative: Better to false alarm than miss real harm")

    # Guardrail: Average ads shown should not decrease significantly
    guardrail_control_ads = control['total_ads'].values
    guardrail_treatment_ads = treatment['total_ads'].values

    results['guardrails'] = {
        'ads_shown': guardrails.non_inferiority_test(
            control=guardrail_control_ads,
            treatment=guardrail_treatment_ads,
            delta=-0.05,  # Allow up to 5% degradation
            metric_type='relative',
            alpha=0.05
        )
    }

    results['guardrails']['ads_shown']['metric_name'] = 'avg_ads_shown'

    if verbose:
        ads_result = results['guardrails']['ads_shown']

        # Calculate relative percentage for display (difference is absolute, need to convert)
        if ads_result.get('metric_type') == 'relative':
            ads_pct_change = ads_result['difference'] / ads_result['mean_control']
        else:
            ads_pct_change = ads_result['difference']  # Already a fraction

        print(f"\n‚úì Guardrail Test Results:")
        print(f"   Metric: Average ads shown per user")
        print(f"   Control mean: {ads_result['mean_control']:.2f} ads")
        print(f"   Treatment mean: {ads_result['mean_treatment']:.2f} ads")
        print(f"   Absolute difference: {ads_result['difference']:.2f} ads")
        print(f"   Relative change: {ads_pct_change:.2%}")
        print(f"   95% CI lower bound: {ads_result['ci_lower']:.4f}")
        print(f"   Non-inferiority margin: {ads_result['margin_used']:.2%}")
        print(f"   Test statistic: {ads_result['t_statistic']:.4f}")
        print(f"   P-value: {ads_result['p_value']:.6f}")

        print(f"\nüí° INTERPRETATION:")
        if ads_result['passed']:
            print(f"   ‚úì GUARDRAIL PASSED")
            print(f"   ‚úì Degradation ({ads_pct_change:.2%}) is within acceptable threshold ({ads_result['margin_used']:.1%})")
            print(f"   ‚úì 95% CI lower bound ({ads_result['ci_lower']:.4f}) > margin ({ads_result['margin_used']:.4f})")
            print(f"   ‚úì Safe to proceed from guardrail perspective")
            print(f"   ")
            print(f"   What this means:")
            print(f"   - Even in worst-case scenario (lower CI bound), degradation < {abs(ads_result['margin_used'])*100:.0f}%")
            print(f"   - This level of harm is acceptable per our threshold")
        else:
            print(f"   ‚úó GUARDRAIL FAILED")
            print(f"   ‚úó Degradation ({ads_pct_change:.2%}) EXCEEDS acceptable threshold ({ads_result['margin_used']:.1%})")
            print(f"   ‚úó 95% CI lower bound ({ads_result['ci_lower']:.4f}) ‚â§ margin ({ads_result['margin_used']:.4f})")
            print(f"   ‚úó TOO RISKY to ship - even if primary metric improved!")
            print(f"   ")
            print(f"   What this means:")
            print(f"   - Cannot rule out harm > {abs(ads_result['margin_used'])*100:.0f}%")
            print(f"   - Unacceptable risk of user experience degradation")
            print(f"   ")
            print(f"   Action items:")
            print(f"   1. Investigate why ads decreased (bug? design issue?)")
            print(f"   2. Redesign treatment to avoid this harm")
            print(f"   3. ABANDON current variant")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Spotify: Uses guardrails on ALL experiments (retention, quality, engagement)")
        print(f"   - Netflix: Tracks 100+ guardrail metrics per experiment")
        print(f"   - Booking.com: Will abandon experiments with ANY critical guardrail failure")
        print(f"   - Common guardrails:")
        print(f"     ‚Ä¢ Revenue/monetization (strict: -1 to -2%)")
        print(f"     ‚Ä¢ User retention (strict: -1%)")
        print(f"     ‚Ä¢ Engagement/time spent (moderate: -5%)")
        print(f"     ‚Ä¢ Technical metrics: latency, errors (very strict: -0.5%)")
        print(f"   - Best practice: Define guardrails BEFORE experiment starts")

    # ========================================================================
    # STEP 6: Novelty Effect Detection
    # ========================================================================
    if verbose:
        print(f"\n[6/8] Detecting novelty effects (time-based analysis)...")
        print("\nüìö LEARNING: Novelty Effects")
        print("   What is a novelty effect?")
        print("   - Users try new features out of curiosity (spike in week 1)")
        print("   - Effect wears off as novelty fades (regression by week 3-4)")
        print("   - Classic example: New UI ‚Üí users explore ‚Üí then ignore it")
        print("   ")
        print("   Why it matters:")
        print("   - Short-term spike ‚â† long-term sustained impact")
        print("   - Shipping novelty effect = wasted engineering resources")
        print("   - Users may actually dislike the feature once novelty wears off")
        print("   ")
        print("   How to detect:")
        print("   - Split experiment into early vs late periods")
        print("   - Compare treatment effect in early vs late windows")
        print("   - Significant decay suggests novelty")
        print("   ")
        print("   What to do if detected:")
        print("   1. Run post-launch holdout (2-4 weeks minimum)")
        print("   2. Monitor: Does effect sustain or return to baseline?")
        print("   3. If effect disappears: ROLL BACK the feature")
        print("   4. Learn: What made users curious but not genuinely engaged?")

    # Aggregate conversion by day of week
    # Note: Marketing dataset doesn't have date, using most_ads_day as proxy
    daily_control = control.groupby('most_ads_day')['converted'].mean().sort_index()
    daily_treatment = treatment.groupby('most_ads_day')['converted'].mean().sort_index()

    if len(daily_control) >= 10:  # Need at least 10 time points
        results['novelty'] = novelty.detect_novelty_effect(
            metrics_control=daily_control.values,
            metrics_treatment=daily_treatment.values,
            window_size=3,  # Use 3 time points for early/late windows
            alpha=0.05
        )

        if verbose:
            print(f"\n‚úì Novelty Analysis Results:")
            print(f"   Time points analyzed: {len(daily_control)}")
            print(f"   Window size: {3} time points for early/late comparison")
            print(f"   ")
            print(f"   Effect Trajectory:")
            print(f"   Early period effect: {results['novelty']['early_effect']:.4f}")
            print(f"   Late period effect: {results['novelty']['late_effect']:.4f}")
            print(f"   Effect decay: {results['novelty']['effect_decay']:.4f}")
            print(f"   Decay significance: p = {results['novelty']['decay_pvalue']:.4f}")

            print(f"\nüí° INTERPRETATION:")
            if results['novelty']['novelty_detected']:
                print(f"   ‚ö†Ô∏è  NOVELTY EFFECT DETECTED (decay p < 0.05)")
                print(f"   ‚ö†Ô∏è  Effect is WEAKENING over time - likely temporary user curiosity")
                print(f"   ‚ö†Ô∏è  Early effect ({results['novelty']['early_effect']:.4f}) > Late effect ({results['novelty']['late_effect']:.4f})")
                print(f"   ")
                print(f"   What this means:")
                print(f"   - Users initially engaged with treatment")
                print(f"   - Engagement declining as experiment progresses")
                print(f"   - Likely represents temporary novelty, not true long-term value")
                print(f"   ")
                print(f"   Recommended actions:")
                print(f"   1. DO NOT ship immediately - even if overall test is significant")
                print(f"   2. Run 2-4 week post-launch holdout (keep 5-10% in control)")
                print(f"   3. Monitor: Does treatment effect stabilize or disappear?")
                print(f"   4. If effect disappears: ROLL BACK feature")
                print(f"   5. Investigate: Why did users lose interest? Design flaw?")
            else:
                print(f"   ‚úì NO NOVELTY EFFECT DETECTED")
                print(f"   ‚úì Effect appears STABLE across experiment duration")
                print(f"   ‚úì Likely represents true long-term impact, not temporary curiosity")
                print(f"   ")
                print(f"   What this means:")
                print(f"   - Treatment effect consistent over time")
                print(f"   - Users continue to engage at same rate")
                print(f"   - Higher confidence this is a real, sustainable improvement")
                print(f"   ")
                print(f"   Recommendation:")
                print(f"   - Still consider short post-launch holdout (1-2 weeks)")
                print(f"   - Validate effect persists in production environment")

            print(f"\nüè¢ INDUSTRY PRACTICE:")
            print(f"   - Statsig: Developed novelty detection framework used by hundreds of companies")
            print(f"   - Netflix: Always runs 4-week minimum experiments for UX changes")
            print(f"   - Facebook: Uses 'time-varying treatment effects' analysis")
            print(f"   - Common finding: 30-50% of 'significant' experiments show novelty decay")
            print(f"   - Best practice: ALWAYS check for novelty on user-facing changes")
            print(f"   - Post-launch holdout: Industry standard for any shipped feature")
    else:
        results['novelty'] = None
        if verbose:
            print(f"\n‚óã Novelty Analysis:")
            print(f"   Insufficient time points ({len(daily_control)}) for novelty detection")
            print(f"   Need ‚â•10 time points for reliable early vs late comparison")
            print(f"   Recommendation: Run longer experiments (2-4 weeks minimum)")

    # ========================================================================
    # STEP 7: Ship/Hold/Abandon Decision
    # ========================================================================
    if verbose:
        print(f"\n[7/8] Making ship/hold/abandon decision...")
        print("\nüìö LEARNING: Decision Framework")
        print("   Ship/Hold/Abandon Logic:")
        print("   ")
        print("   SHIP = Launch to all users")
        print("   ‚úì Primary metric: Significant AND positive")
        print("   ‚úì ALL guardrails: Passed")
        print("   ‚úì No critical risks identified")
        print("   ‚Üí Confidence: Treatment improves experience without harm")
        print("   ")
        print("   ABANDON = Kill this variant")
        print("   ‚úó Primary metric: Significant AND negative, OR")
        print("   ‚úó ANY critical guardrail: Failed")
        print("   ‚Üí Confidence: Treatment causes harm")
        print("   ")
        print("   HOLD = Inconclusive - need more investigation")
        print("   ‚óã Primary metric: Not significant (could go either way)")
        print("   ‚óã OR: Mixed signals (some good, some concerning)")
        print("   ‚Üí Options:")
        print("     1. Extend experiment (collect more data)")
        print("     2. Investigate unexpected patterns")
        print("     3. Iterate on treatment design")
        print("   ")
        print("   Beyond statistics:")
        print("   - Statistical significance ‚â† practical significance")
        print("   - Consider: Implementation cost, opportunity cost, risks")
        print("   - Ask: Is the juice worth the squeeze?")

    guardrail_list = [results['guardrails']['ads_shown']]

    results['decision'] = guardrails.evaluate_guardrails(
        primary_result=results['primary_test'],
        guardrail_results=guardrail_list
    )

    if verbose:
        decision = results['decision']['decision'].upper()

        print(f"\n‚úì Decision Matrix:")
        print(f"   Primary metric significant: {results['decision']['primary_significant']}")
        print(f"   Primary metric positive: {results['decision']['primary_positive']}")
        print(f"   Guardrails passed: {results['decision']['guardrails_passed']} / {results['decision']['guardrails_total']}")
        if results['novelty'] and results['novelty']['novelty_detected']:
            print(f"   Novelty risk: ‚ö†Ô∏è  YES - effect may decay post-launch")
        else:
            print(f"   Novelty risk: ‚úì No - effect appears stable")
        print(f"")
        print(f"   >>> FINAL DECISION: {decision} <<<")
        print(f"")

        print(f"\nüí° INTERPRETATION:")
        if decision == 'SHIP':
            print(f"   ‚úì SHIP TO ALL USERS")
            print(f"   ")
            print(f"   Why ship?")
            print(f"   - Primary metric improved significantly ({results['primary_test']['relative_lift']:.2%} lift)")
            print(f"   - All guardrails passed (no unacceptable harm)")
            print(f"   - Statistical evidence is strong (p = {results['primary_test']['p_value']:.4f})")
            print(f"   ")
            print(f"   Expected impact:")
            print(f"   - {results['primary_test']['relative_lift']:.2%} improvement in conversions")
            print(f"   - For every 1,000 users: {results['primary_test']['absolute_lift']*1000:.1f} more conversions")
            print(f"   - (See Step 8 below for full revenue projections)")
            print(f"   ")
            if results['novelty'] and results['novelty']['novelty_detected']:
                print(f"   ‚ö†Ô∏è  CAUTION: Novelty effect detected")
                print(f"   ‚ö†Ô∏è  Recommendation: Run 2-4 week post-launch holdout")
                print(f"   ‚ö†Ô∏è  Keep 5-10% of users in control to validate sustained effect")
                print(f"   ")
            print(f"   Next steps:")
            print(f"   1. Prepare rollout plan (gradual vs immediate?)")
            print(f"   2. Setup monitoring dashboards")
            print(f"   3. Define rollback criteria")
            if results['novelty'] and results['novelty']['novelty_detected']:
                print(f"   4. Configure post-launch holdout (2-4 weeks)")
            print(f"   5. Communicate results to stakeholders")

        elif decision == 'ABANDON':
            print(f"   ‚úó ABANDON THIS VARIANT")
            print(f"   ")
            print(f"   Why abandon?")
            if not results['decision']['primary_positive']:
                print(f"   - Primary metric showed NEGATIVE impact ({results['primary_test']['relative_lift']:.2%})")
                print(f"   - Treatment is actively harming user experience")
            if results['decision']['guardrails_passed'] < results['decision']['guardrails_total']:
                print(f"   - {results['decision']['guardrails_total'] - results['decision']['guardrails_passed']} guardrail(s) FAILED")
                print(f"   - Unacceptable degradation in critical metrics")
            print(f"   ")
            print(f"   What we learned:")
            print(f"   - This approach doesn't work - that's valuable information!")
            print(f"   - Failing fast saved resources vs shipping bad experience")
            print(f"   ")
            print(f"   Next steps:")
            print(f"   1. Conduct post-mortem: Why didn't it work?")
            print(f"   2. Analyze user feedback, behavior patterns")
            print(f"   3. Generate new hypotheses")
            print(f"   4. Design improved variant for next iteration")
            print(f"   5. Remember: Most experiments 'fail' - iteration is key")

        else:  # HOLD
            print(f"   ‚óã HOLD - INCONCLUSIVE")
            print(f"   ")
            print(f"   Why hold?")
            if not results['decision']['primary_significant']:
                print(f"   - Primary metric not statistically significant (p = {results['primary_test']['p_value']:.4f})")
                print(f"   - Observed {results['primary_test']['relative_lift']:.2%} lift could be random noise")
                print(f"   - Need more data to reach definitive conclusion")
            print(f"   ")
            print(f"   Options to consider:")
            print(f"   ")
            print(f"   1. EXTEND EXPERIMENT (most common)")
            print(f"      - Run 1-2 more weeks to increase sample size")
            print(f"      - Current power: Check if we can detect MDE={results['power_analysis']['mde_relative']:.1%}")
            print(f"      - Needed: {results['power_analysis']['sample_per_group']:,} per group")
            print(f"      - Have: {len(control):,} per group")
            print(f"   ")
            print(f"   2. INCREASE TRAFFIC ALLOCATION")
            print(f"      - Give more users to experiment (50/50 ‚Üí 80% treatment)")
            print(f"      - Faster data collection")
            print(f"      - Trade-off: Higher risk if treatment is actually harmful")
            print(f"   ")
            print(f"   3. REVISE HYPOTHESIS")
            print(f"      - Maybe targeting wrong user segment?")
            print(f"      - Test on specific cohorts with stronger expected effect")
            print(f"   ")
            print(f"   4. ACCEPT & MOVE ON")
            print(f"      - If multiple extensions still inconclusive: no effect exists")
            print(f"      - Engineering time better spent elsewhere")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Booking.com: Ships 25,000+ experiments/year, ~10-30% 'win rate'")
        print(f"   - Netflix: 90% of experiments show 'no effect' - that's normal!")
        print(f"   - Microsoft: Uses decision trees with 10+ criteria beyond p-value")
        print(f"   - Common mistake: Shipping marginally significant results (p~0.04)")
        print(f"     ‚Ä¢ Should be decisively significant (p < 0.01) for high-impact changes")
        print(f"   - Best practice: Pre-commit to decision criteria before experiment")
        print(f"   - Reality: Most 'Hold' decisions eventually become 'Abandon' after iteration")

    # ========================================================================
    # STEP 8: Business Impact Calculation
    # ========================================================================
    if verbose:
        print(f"\n[8/8] Calculating business impact...")
        print("\nüìö LEARNING: Translating Statistics to Business Value")
        print("   Why business impact matters:")
        print("   - P-values tell us 'is it real?' (statistical significance)")
        print("   - Business impact tells us 'does it matter?' (practical significance)")
        print("   - Can have p < 0.001 but trivial business value (waste of resources)")
        print("   - Can have p ~ 0.03 but huge business value (worth the risk)")
        print("   ")
        print("   Components of business impact:")
        print("   1. Scale: How many users affected?")
        print("   2. Magnitude: How big is the effect per user?")
        print("   3. Value: What's each conversion/action worth?")
        print("   4. Time: Annualized value for comparisons")
        print("   5. Costs: Implementation cost, opportunity cost, maintenance")
        print("   ")
        print("   Decision formula:")
        print("   Expected Value = (Lift √ó Users √ó Value/User √ó Probability) - Costs")
        print("   - Probability: Confidence from p-value")
        print("   - Account for: CI bounds (best/worst case scenarios)")
        print("   ")
        print("   Reality checks:")
        print("   - Are assumptions realistic? (user growth, retention, AOV)")
        print("   - Are we double-counting? (cannibalizing other features)")
        print("   - What about long-term effects? (LTV, churn)")

    # Assume 1M monthly active users and $10 average order value
    monthly_users = 1_000_000
    avg_order_value = 10.0

    baseline_conversions_monthly = monthly_users * p_control
    treatment_conversions_monthly = monthly_users * results['data_summary']['conversion_rate_treatment']
    incremental_conversions_monthly = treatment_conversions_monthly - baseline_conversions_monthly

    # Calculate best/worst case using CI bounds
    # IMPORTANT: ci_lower/ci_upper are confidence intervals for the DIFFERENCE (incremental lift),
    # NOT for the treatment rate. They already represent incremental conversions per user.
    # So we multiply by users and value directly, without subtracting baseline again.
    worst_case_incremental_conversions = monthly_users * results['primary_test']['ci_lower']
    best_case_incremental_conversions = monthly_users * results['primary_test']['ci_upper']
    worst_case_incremental = worst_case_incremental_conversions * avg_order_value * 12
    best_case_incremental = best_case_incremental_conversions * avg_order_value * 12

    results['business_impact'] = {
        'monthly_users': monthly_users,
        'avg_order_value': avg_order_value,
        'baseline_conversions_monthly': baseline_conversions_monthly,
        'treatment_conversions_monthly': treatment_conversions_monthly,
        'incremental_conversions_monthly': incremental_conversions_monthly,
        'incremental_revenue_monthly': incremental_conversions_monthly * avg_order_value,
        'incremental_revenue_annual': incremental_conversions_monthly * avg_order_value * 12,
        'worst_case_annual': worst_case_incremental,
        'best_case_annual': best_case_incremental,
    }

    if verbose:
        print(f"\n‚úì Business Impact Calculation:")
        print(f"   ")
        print(f"   üìä ASSUMPTIONS (customize these for your business):")
        print(f"   - Monthly active users: {monthly_users:,}")
        print(f"   - Average order value (AOV): ${avg_order_value:.2f}")
        print(f"   - Baseline conversion rate: {p_control:.2%}")
        print(f"   - Treatment conversion rate: {results['data_summary']['conversion_rate_treatment']:.2%}")
        print(f"   - Relative lift: {results['primary_test']['relative_lift']:.2%}")
        print(f"   ")
        print(f"   üìà PROJECTED MONTHLY IMPACT:")
        print(f"   - Baseline conversions: {baseline_conversions_monthly:,.0f}")
        print(f"   - Treatment conversions: {treatment_conversions_monthly:,.0f}")
        print(f"   - Incremental conversions: {incremental_conversions_monthly:,.0f}")
        print(f"   - Incremental revenue: ${results['business_impact']['incremental_revenue_monthly']:,.2f}")
        print(f"   ")
        print(f"   üí∞ ANNUALIZED IMPACT (point estimate):")
        print(f"   - Incremental revenue/year: ${results['business_impact']['incremental_revenue_annual']:,.2f}")
        print(f"   - Per user value: ${results['business_impact']['incremental_revenue_annual']/monthly_users:.2f}/year")
        print(f"   ")
        print(f"   üìä CONFIDENCE INTERVAL RANGE (95% CI):")
        print(f"   - Best case (upper CI): ${results['business_impact']['best_case_annual']:,.2f}/year")
        print(f"   - Expected (point estimate): ${results['business_impact']['incremental_revenue_annual']:,.2f}/year")
        print(f"   - Worst case (lower CI): ${results['business_impact']['worst_case_annual']:,.2f}/year")

        print(f"\nüí° INTERPRETATION:")
        if results['decision']['decision'].upper() == 'SHIP':
            print(f"   ‚úì EXPECTED ANNUAL VALUE: ${results['business_impact']['incremental_revenue_annual']:,.2f}")
            print(f"   ")
            print(f"   Cost-benefit analysis:")
            print(f"   - If implementation cost < ${results['business_impact']['incremental_revenue_annual']:,.0f}:")
            print(f"     ‚Üí ROI positive in first year - STRONG SHIP signal")
            print(f"   - If implementation cost = ${results['business_impact']['incremental_revenue_annual']*0.5:,.0f}:")
            print(f"     ‚Üí Break-even in ~6 months - GOOD investment")
            print(f"   - If implementation cost > ${results['business_impact']['incremental_revenue_annual']:,.0f}:")
            print(f"     ‚Üí Need multi-year value or strategic reasons to justify")
            print(f"   ")
            print(f"   Risk assessment:")
            print(f"   - Worst case (95% CI lower): ${results['business_impact']['worst_case_annual']:,.2f}/year")
            if results['business_impact']['worst_case_annual'] > 0:
                print(f"   - ‚úì Even in worst case, still profitable")
                print(f"   - ‚úì Low downside risk")
            else:
                print(f"   - ‚ö†Ô∏è  Worst case is negative")
                print(f"   - ‚ö†Ô∏è  Consider: Is upside worth the downside risk?")
            print(f"   ")
            print(f"   Opportunity cost:")
            print(f"   - Engineering time to build this: ??? days")
            print(f"   - Could that time create more value elsewhere?")
            print(f"   - Is this the highest-leverage experiment to ship?")
        else:
            print(f"   ‚óã Business impact not applicable - experiment did not ship")
            print(f"   ")
            print(f"   Value of running this experiment:")
            print(f"   - Learned this approach doesn't work - valuable insight!")
            print(f"   - Avoided waste: NOT shipping bad experience")
            print(f"   - Cost: Experiment runtime, engineering, opportunity cost")
            print(f"   - Next: Use learnings to design better variant")

        print(f"\nüè¢ INDUSTRY PRACTICE:")
        print(f"   - Amazon: Requires business case showing ROI > threshold before shipping")
        print(f"   - Booking.com: Tracks cumulative impact: 1000+ wins = massive growth")
        print(f"   - Netflix: Small wins compound - focus on velocity, not just big bets")
        print(f"   - Example: 10 experiments √ó 2% lift each = 21.9% total growth")
        print(f"     (1.02^10 - 1 = 0.219, due to compounding)")
        print(f"   ")
        print(f"   Common mistakes:")
        print(f"   - Ignoring implementation cost (time, complexity, maintenance)")
        print(f"   - Over-estimating user growth (assuming scale that doesn't materialize)")
        print(f"   - Not accounting for cannibalization (stealing from other features)")
        print(f"   - Forgetting about reversibility cost (how easy to roll back?)")
        print(f"   ")
        print(f"   Best practice:")
        print(f"   - Use conservative assumptions (avoid over-optimism)")
        print(f"   - Show range (best/expected/worst case)")
        print(f"   - Account for costs explicitly")
        print(f"   - Update projections post-launch (validate assumptions)")

    # ========================================================================
    # Pipeline Complete
    # ========================================================================
    if verbose:
        print(f"\n{'='*70}")
        print(f"ANALYSIS COMPLETE")
        print(f"{'='*70}\n")

    return results


if __name__ == '__main__':
    # Fix Windows terminal encoding for emoji support
    import sys
    import io
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("\n" + "="*80)
    print("A/B TESTING MASTERCLASS: MARKETING CAMPAIGN ANALYSIS")
    print("="*80)
    print("\nThis pipeline demonstrates a complete A/B testing workflow:")
    print("  1. Data quality validation and group balance checks")
    print("  2. Sample Ratio Mismatch (SRM) detection")
    print("  3. Power analysis and statistical hypothesis testing")
    print("  4. Variance reduction with CUPED")
    print("  5. Guardrail metric evaluation")
    print("  6. Novelty effect detection (time-based analysis)")
    print("  7. Ship/hold/abandon decision framework")
    print("  8. Business impact translation")
    print("\n" + "="*80)
    print("Starting analysis...")
    print("="*80 + "\n")

    # Run the full pipeline
    results = run_marketing_analysis(sample_frac=0.1, verbose=True)

    # Print comprehensive summary
    print("\n" + "="*70)
    print("üéØ FINAL SUMMARY & KEY LEARNINGS")
    print("="*70)

    print(f"\n1. EXPERIMENT OVERVIEW")
    print(f"   Dataset: Marketing A/B Test ({results['data_summary']['total_observations']:,} observations)")
    print(f"   Date range: {results['data_summary']['date_range']}")
    print(f"   Groups: {results['data_summary']['control_size']:,} control (PSA), {results['data_summary']['treatment_size']:,} treatment (Ad)")
    print(f"   Primary metric: Conversion rate")

    print(f"\n2. DATA QUALITY ‚úì")
    print(f"   SRM check: {'‚úì PASSED' if not results['srm_check']['srm_detected'] else '‚úó FAILED'}")
    if not results['srm_check']['srm_detected']:
        print(f"   - Randomization is valid (p = {results['srm_check']['p_value']:.4f})")
        print(f"   - Safe to trust results")
    else:
        print(f"   - ‚ö†Ô∏è  WARNING: Randomization failure detected!")
        print(f"   - ‚ö†Ô∏è  All results below are INVALID - investigate immediately")

    print(f"\n3. STATISTICAL RESULTS")
    print(f"   Control conversion: {results['data_summary']['conversion_rate_control']:.2%}")
    print(f"   Treatment conversion: {results['data_summary']['conversion_rate_treatment']:.2%}")
    print(f"   Absolute difference: {results['primary_test']['absolute_lift']:.4%} ({results['primary_test']['absolute_lift']*100:.2f} percentage points)")
    print(f"   Relative lift: {results['primary_test']['relative_lift']:.2%}")
    print(f"   P-value: {results['primary_test']['p_value']:.6f}")
    print(f"   95% CI: [{results['primary_test']['ci_lower']:.4%}, {results['primary_test']['ci_upper']:.4%}]")
    print(f"   Statistical significance: {'‚úì YES (p < 0.05)' if results['primary_test']['significant'] else '‚óã NO (p ‚â• 0.05)'}")
    print(f"   ")
    print(f"   Plain English:")
    if results['primary_test']['significant']:
        print(f"   - Treatment shows a REAL {results['primary_test']['relative_lift']:.2%} improvement")
        print(f"   - For every 1,000 users, expect {results['primary_test']['absolute_lift']*1000:.1f} more conversions")
        print(f"   - Confidence: 95% (only {results['primary_test']['p_value']*100:.2f}% chance this is random)")
    else:
        print(f"   - No statistically significant difference detected")
        print(f"   - Either no real effect, or sample too small")

    print(f"\n4. POWER ANALYSIS")
    print(f"   Baseline: {results['power_analysis']['p_baseline']:.2%}")
    print(f"   Minimum Detectable Effect (MDE): {results['power_analysis']['mde_relative']:.1%} relative")
    print(f"   Required sample: {results['power_analysis']['sample_per_group']:,} per group")
    print(f"   Actual sample: ~{results['data_summary']['control_size']:,} per group")
    if results['data_summary']['control_size'] >= results['power_analysis']['sample_per_group']:
        print(f"   Status: ‚úì Well-powered ({results['data_summary']['control_size']/results['power_analysis']['sample_per_group']:.1f}x required)")
    else:
        print(f"   Status: ‚ö†Ô∏è  Underpowered ({results['data_summary']['control_size']/results['power_analysis']['sample_per_group']:.1%} of required)")

    print(f"\n5. VARIANCE REDUCTION (CUPED)")
    print(f"   Covariate: total_ads (pre-experiment exposure)")
    print(f"   Correlation: {results['cuped']['correlation']:.4f}")
    print(f"   Variance reduction: {results['cuped']['var_reduction']:.1%}")
    print(f"   Equivalent sample gain: {results['cuped']['sample_size_reduction']:.1%}")
    if results['cuped']['var_reduction'] > 0.20:
        print(f"   Effectiveness: ‚úì STRONG - like running with {1/(1-results['cuped']['var_reduction']):.1f}x more users")
    elif results['cuped']['var_reduction'] > 0.10:
        print(f"   Effectiveness: ‚úì MODERATE - helpful noise reduction")
    else:
        print(f"   Effectiveness: ‚óã WEAK - covariate not very predictive")

    print(f"\n6. GUARDRAIL METRICS")
    guardrails_passed = results['decision']['guardrails_passed']
    guardrails_total = results['decision']['guardrails_total']
    print(f"   Total checked: {guardrails_total}")
    print(f"   Passed: {guardrails_passed}")
    print(f"   Status: {'‚úì ALL CLEAR' if guardrails_passed == guardrails_total else '‚úó FAILURES DETECTED'}")
    for name, gr in results['guardrails'].items():
        status = '‚úì PASS' if gr['passed'] else '‚úó FAIL'
        print(f"   - {gr.get('metric_name', name)}: {status} ({gr['difference']:.2%} change, margin: {gr['margin_used']:.1%})")

    print(f"\n7. NOVELTY EFFECTS")
    if results['novelty'] is not None:
        if results['novelty']['novelty_detected']:
            print(f"   Status: ‚ö†Ô∏è  DETECTED (effect decaying over time)")
            print(f"   Early effect: {results['novelty']['early_effect']:.4f}")
            print(f"   Late effect: {results['novelty']['late_effect']:.4f}")
            print(f"   Decay: {results['novelty']['effect_decay']:.4f} (p = {results['novelty']['decay_pvalue']:.4f})")
            print(f"   Warning: Effect may be temporary curiosity, not sustainable value")
            print(f"   Recommendation: Run post-launch holdout to validate")
        else:
            print(f"   Status: ‚úì NO NOVELTY (effect stable across time)")
            print(f"   Effect appears to be sustained, not temporary spike")
    else:
        print(f"   Status: ‚óã NOT ANALYZED (insufficient time points)")

    print(f"\n8. FINAL DECISION")
    decision = results['decision']['decision'].upper()
    print(f"   >>> {decision} <<<")
    print(f"   ")
    if decision == 'SHIP':
        print(f"   ‚úì Ship this treatment to all users")
        print(f"   ‚úì Primary metric improved AND all guardrails passed")
        print(f"   ‚úì Expected value: ${results['business_impact']['incremental_revenue_annual']:,.0f}/year")
        if results['novelty'] and results['novelty']['novelty_detected']:
            print(f"   ‚ö†Ô∏è  But: Monitor closely - novelty effect detected")
    elif decision == 'HOLD':
        print(f"   ‚óã Inconclusive - need more data or investigation")
        print(f"   ‚óã Primary metric not significant OR mixed signals")
        print(f"   ‚óã Options: Extend experiment, iterate design, or abandon")
    else:
        print(f"   ‚úó Abandon this variant")
        print(f"   ‚úó Treatment shows negative impact or critical failures")
        print(f"   ‚úì But: Learned this doesn't work - valuable insight!")

    print(f"\n9. BUSINESS IMPACT (if shipped)")
    print(f"   Expected (point estimate): ${results['business_impact']['incremental_revenue_annual']:,.2f}/year")
    print(f"   Best case (95% CI upper): ${results['business_impact']['best_case_annual']:,.2f}/year")
    print(f"   Worst case (95% CI lower): ${results['business_impact']['worst_case_annual']:,.2f}/year")
    print(f"   ")
    print(f"   Assumptions:")
    print(f"   - {results['business_impact']['monthly_users']:,} monthly active users")
    print(f"   - ${results['business_impact']['avg_order_value']:.2f} average order value")
    print(f"   - {results['primary_test']['relative_lift']:.2%} sustained lift")

    print(f"\n" + "="*70)
    print(f"üìö KEY TAKEAWAYS FOR A/B TESTING")
    print(f"="*70)
    print(f"\n‚úì ALWAYS CHECK THESE (in order):")
    print(f"  1. Data quality: Missing values, outliers, group balance")
    print(f"  2. SRM first: If randomization failed, STOP - all results invalid")
    print(f"  3. Power analysis: Did we collect enough data to detect MDE?")
    print(f"  4. Statistical test: Is the effect real (p < 0.05)?")
    print(f"  5. Effect size: Is it practically meaningful (not just significant)?")
    print(f"  6. Guardrails: Did we harm anything important?")
    print(f"  7. Novelty: Will this effect last or fade?")
    print(f"  8. Business value: Is the ROI worth the cost?")

    print(f"\n‚ö†Ô∏è  COMMON PITFALLS TO AVOID:")
    print(f"  1. P-hacking: Stopping as soon as p < 0.05 (pre-commit to sample size)")
    print(f"  2. Ignoring SRM: Trusting results from bad randomization")
    print(f"  3. Optimizing primary at expense of guardrails")
    print(f"  4. Shipping novelty effects (temporary spikes, not real value)")
    print(f"  5. Confusing statistical significance with practical significance")
    print(f"  6. Not accounting for multiple testing (running 100 tests ‚Üí 5 false positives)")
    print(f"  7. Underpowered experiments (too small to detect realistic effects)")

    print(f"\nüè¢ INDUSTRY BEST PRACTICES:")
    print(f"  ‚Ä¢ Booking.com: 25,000+ experiments/year, 90%+ properly powered")
    print(f"  ‚Ä¢ Netflix: Always checks SRM, runs 2-4 week minimum for UX changes")
    print(f"  ‚Ä¢ Microsoft: Automated SRM monitoring, strict guardrail framework")
    print(f"  ‚Ä¢ Spotify: Uses CUPED on all experiments, ~30% average variance reduction")
    print(f"  ‚Ä¢ Reality: 70-90% of experiments show 'no effect' - that's NORMAL")
    print(f"  ‚Ä¢ Success: Small wins compound - 10 √ó 2% lifts = 22% total growth")

    print(f"\nüí° WHAT TO DO NEXT:")
    print(f"  1. Review this pipeline's methodology - understand each step")
    print(f"  2. Run with different sample_frac values to see stability")
    print(f"  3. Try modifying parameters (alpha, MDE, guardrail thresholds)")
    print(f"  4. Apply this framework to YOUR experiments")
    print(f"  5. Build experimentation culture: Iterate, learn, compound wins")

    print(f"\n" + "="*70)
    print(f"‚úÖ Pipeline complete! You now understand end-to-end A/B testing.")
    print(f"="*70 + "\n")
